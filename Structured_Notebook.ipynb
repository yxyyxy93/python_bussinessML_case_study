{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Introduction ](#1.-Introduction )\n",
    "* [2. Set-up ](#2.-Set-up )\n",
    "    * [2.1. Installs and imports of libraries](#2.1.-Installs-and-imports-of-libraries)\n",
    "    * [2.2. Import dataset](#2.2.-Import-dataset)\n",
    "    * [2.3. Clean the data](#2.3.-Clean-the-data)\n",
    "* [3. Helper Function and Key Variables](#3.-Helper-Function-and-Key-Variables)\n",
    "    * [3.1. Add Dependent Variable](#3.1.-Add-Dependent-Variable)\n",
    "    * [3.2. Unique Artist Names, etc, etc,](#3.2.-Unique-Artist-Names,-etc,-etc,)\n",
    "    * [1.3. Connection to **`Spotify API`**](#3.3.-Connection-to-Spotify-API)\n",
    "* [4. Exploratory Data Analysis](#4.-Exploratory-Data-Analysis)\n",
    "    * [4.1 Initial Overview of Data Load](#4.1-INITIAL-OVERVIEW-OF-DATA-LOAD)\n",
    "    * [4.2 Age, Stream Length, Hour, Weekday Distribution](#4.2-AGE,-STREAM-LENGTH,-HOUR,-WEEKDAY-DISTRIBUTION)\n",
    "    * [4.3 User-based Features Distribution](#4.3-USER--BASED-FEATURES-DISTRIBUTION)\n",
    "    * [4.4 Time Series Plot](#4.4-TIME-SERIES-PLOT)\n",
    "    * [4.5 Artists and Playlists Plots](#4.5-ARTISTS-AND-PLAYLISTS-PLOTS)\n",
    "    * [4.6 Key Playlists Vizualizations](#4.6-KEY-PLAYLISTS-VIZUALIZATIONS )\n",
    "* [5. Feature Engineering](#5.-Feature-Engineering)\n",
    "    * [5.1 Artist Features](#5.1-Artist-Features)\n",
    "    * [5.2 Playlist Features](#5.2-Playlist-Features)\n",
    "    * [5.3 User Features](#5.3-User-Features)\n",
    "    * [5.4 Merge the features into the analytics-ready DataFrame](#5.4-Merge-the-features-into-the-analytics-ready-DataFrame)\n",
    "* [6. Data Preparation](#6.-Data-Preparation)\n",
    "    * [6.1 Dealing with Missing Data](#6.1-Deal-with-Missing-Data)\n",
    "    * [6.2 Feature Correlations](#6.2-Feature-Correlations)  \n",
    "    * [6.3 Class Imbalance](#6.3-Class-Imbalance)\n",
    "    * [6.4 Train-Test Split](#6.4-Train-Test-Split)\n",
    "    * [6.5 Feature Scaling](#6.5-Feature-Scaling)\n",
    "* [7. Model Generation (Classical)](#7.-Model-Generation-(Classical))\n",
    "    * [7.1 Class Imbalance Problem](#7.1-Class-Imbalance-Problem)\n",
    "    * [7.2 Logistic Regression](#7.2-Logistic-Regression)\n",
    "    * [7.3 Naive Bayes](#7.3-Naive-Bayes)\n",
    "    * [7.4 Support Vector Machine](#7.4-Support-Vector-Machine)\n",
    "    * [7.5 Decision Tree](#7.5-Decision-Tree)  \n",
    "    * [7.6 Ensemble Learning](#7.6-Ensemble-Learning)\n",
    "        * [7.6.1 Random Forest](#7.6.1-Random-Forest)\n",
    "        * [7.6.2 Adaboost](#7.6.2-Adaboost)\n",
    "        * [7.6.3 Soft Voting Ensemble Model](#7.6.3-Sof-Voting-Ensemble-Model)\n",
    "* [8. Extra Models (Deep Learning)](#7.-Extra-Models-(Deep-Learning))\n",
    "    * [8.1 Neural Network](#8.1-Neural-Network)\n",
    "    * [8.2 Knowledge Transfer](#8.2-Knowledge-Transfer)\n",
    "* [9. Model Evaluation and Selection](#9.-Model-Evaluation-and-Selection)\n",
    "    * [9.1 Confusion Matrix, Accuracy, Recall, etc](#9.1-Confusion-Matrix,-Accuracy,-Recall,-etc)\n",
    "    * [9.2 ROC Curve](#9.2-ROC-Curve)\n",
    "    * [9.3 Feature Importance](#9.3-Feature-Importance)\n",
    "* [10. Conclusions and Recommendations](#10.-Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set up\n",
    "## 2.1. Installs and imports of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: missingno in d:\\anaconda\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from missingno) (1.18.1)\n",
      "Requirement already satisfied: seaborn in d:\\anaconda\\lib\\site-packages (from missingno) (0.10.0)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from missingno) (1.3.1)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (from missingno) (3.1.3)\n",
      "Requirement already satisfied: pandas>=0.22.0 in d:\\anaconda\\lib\\site-packages (from seaborn->missingno) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib->missingno) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->missingno) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->missingno) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->missingno) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\anaconda\\lib\\site-packages (from pandas>=0.22.0->seaborn->missingno) (2019.3)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from cycler>=0.10->matplotlib->missingno) (1.14.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->missingno) (45.1.0.post20200119)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0.post20200119)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "Requirement already up-to-date: spotipy in d:\\anaconda\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in d:\\anaconda\\lib\\site-packages (from spotipy) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: requests>=2.20.0 in d:\\anaconda\\lib\\site-packages (from spotipy) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.20.0->spotipy) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.20.0->spotipy) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.20.0->spotipy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests>=2.20.0->spotipy) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "!pip install missingno\n",
    "import missingno as msno\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "!pip install matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import custom functions from library, named 'spotfunc'\n",
    "import spotfunc as spotfunc_v2\n",
    "\n",
    "# Ignore useless warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# Various other things needed for data preparation\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Imports for Principal Component Analysis (PCA)\n",
    "!\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For the Spotify API\n",
    "!pip install spotipy --upgrade\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read in sampled data\n",
    "data = pd.read_csv('cleaned_data.csv', low_memory=False)\n",
    "print('rows data:',len(data))\n",
    "\n",
    "# Keep a copy of original data in case of changes made to dataframe\n",
    "all_artists = data.copy()\n",
    "\n",
    "# Load playlist data\n",
    "playlist_ids_and_titles = pd.read_csv('playlists_ids_and_titles.csv',encoding = 'latin-1',error_bad_lines=False,warn_bad_lines=False)\n",
    "\n",
    "# Drop duplicates\n",
    "playlist_mapper = playlist_ids_and_titles.drop_duplicates(['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-14aebcf2e4ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#DATA CLEANING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#make all artists with lower case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'artist_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'artist_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'track_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'track_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#DATA CLEANING \n",
    "#make all artists with lower case\n",
    "data['artist_name'] = data['artist_name'].astype(str).str.lower()\n",
    "data['track_name'] = data['track_name'].astype(str).str.lower()\n",
    "\n",
    "# As age is more intuitive to interpret than birthyear, we convert \n",
    "data[\"user_age\"] = 2016-data[\"birth_year\"]\n",
    "data['user_age'] = data['user_age'].apply(lambda x: x if x <= 100 else 100) #Ioana: anything greater than 100 will take the value of 100\n",
    "# Maybe instead of making <100 to 100 maybe make them the median or mean or mode?\n",
    "\n",
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df contianing only the 4 popular playlists and all their associated IDs\n",
    "filtered_mapper = playlist_mapper[playlist_mapper[\"name\"].isin([\"Hot Hits UK\", \"Massive Dance Hits\", \"The Indie List\", \"New Music Friday\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant columns\n",
    "data.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'], axis=1, inplace = True) # not bringing any value\n",
    "data.drop(['referral_code' , 'offline_timestamp', 'stream_cached', 'source'], axis=1, inplace = True) # null columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Add Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select relevant playlists \n",
    "successid=filtered_mapper[\"id\"].unique().tolist()\n",
    "\n",
    "# Define variable which looks at \"if artist && top playlist ==> successful, otherwise unsuccessful\"\n",
    "success = data[\"playlist_id\"].isin(successid)\n",
    "data[\"successful\"] = success\n",
    "\n",
    "# TZ - Just addded, need to figure out where to put these. Needed for feature engineering\n",
    "list_fourkeyplaylists = ['Hot Hits UK', 'Massive Dance Hits', 'The Indie List', 'New Music Friday']\n",
    "list_uniqueartists = list(data['artist_name'].unique())\n",
    "df_withoutkeyplaylists = data[~data['playlist_name'].isin(list_fourkeyplaylists)]\n",
    "\n",
    "#============second variable==============\n",
    "# Functon that retrieves a list with all successful artists (who showed up on a top playlist at least once)\n",
    "def get_successful_artists(all_artists):\n",
    "    records_successful_playlists = all_artists[all_artists['playlist_id'].isin(successid)]\n",
    "    return records_successful_playlists['artist_name'].unique()\n",
    "\n",
    "successful_artists = get_successful_artists(data)\n",
    "\n",
    "# Define variable which looks at \"if successful artist ==> successful, otherwise unsuccessful\"\n",
    "# Create column with dependent variable \"success\", which takes value of 1if the artist is in the successful list\n",
    "data['success'] = np.where(data['artist_name'].isin(successful_artists), 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can have successful artists in both buckets, but only first is successfull artists in successfull playlists\n",
    "print('successful artists:', data.loc[data['successful'] == 1]['artist_name'].nunique())\n",
    "print('unsuccessful artists:', data.loc[data['successful'] == 0]['artist_name'].nunique())\n",
    "data['successful'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If artists is successfull, regardless of playlist, its 1\n",
    "print('successful artists:', data.loc[data['success'] == 1]['artist_name'].nunique())\n",
    "print('unsuccessful artists:', data.loc[data['success'] == 0]['artist_name'].nunique())\n",
    "data['success'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR PLOTTING CLUSTER MATRIX MAPPING\n",
    "def plot_cluster(t,figsize=None):\n",
    "    cg = sns.clustermap(t,figsize=(10, 8), cmap=\"mako\", vmin=1)\n",
    "    plt.setp(cg.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "    return cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Unique Artist Names, etc, etc,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TZ - Takes the main Pandas dataframe and returns a list of the unique artist names\n",
    "def get_list_unique_artists(data):\n",
    "    all_artists = pd.DataFrame(data['artist_name'].unique())\n",
    "    all_artists.rename(columns={0: \"artist_name\"},inplace=True)\n",
    "    return all_artists['artist_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TZ - Flattens contents of a nested list inside a Pandas dataframe column and creates a new row with each of the\n",
    "# list values\n",
    "def expand_list(df, list_column, new_column): \n",
    "    lens_of_lists = df[list_column].apply(len)\n",
    "    origin_rows = range(df.shape[0])\n",
    "    destination_rows = np.repeat(origin_rows, lens_of_lists)\n",
    "    non_list_cols = (\n",
    "      [idx for idx, col in enumerate(df.columns)\n",
    "       if col != list_column]\n",
    "    )\n",
    "    expanded_df = df.iloc[destination_rows, non_list_cols].copy()\n",
    "    expanded_df[new_column] = (\n",
    "      [item for items in df[list_column] for item in items]\n",
    "      )\n",
    "    expanded_df.reset_index(inplace=True, drop=True)\n",
    "    return expanded_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Connection to Spotify API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code that connects to the Spotify API, to be used to get the genre information for artists.\n",
    "#Credentials that provide access to the Spotify API\n",
    "client_id='85b60901e29e4a00bb292f0376dfcf7d'\n",
    "client_secret='4f25b06efa8a46608fd17a9631c2c32a'\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)\n",
    "\n",
    "# Create Spotify object to access API\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis (DON'T RUN THIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. INITIAL OVERVIEW OF DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we look at what data type Pandas attributed to each cloumn\n",
    "# we need two options to force Pandas to count non-null instances in each coloumn\n",
    "data.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b> Initial Overview of data load </b>\n",
    "\n",
    "<p>The first thing we observe is that the majority (28/45) of columns are non-numeric (neither int nor float Dtypes), and got classified as an object instead. While an object could, in theory, hold different values, we know that these objects hold text values due to the fact that they came from a .csv file (which can only hold text or numbers).\n",
    "    \n",
    "With that out of the way, we see that not all columns contain 3805499 non-null values (e.g. postal_code only has 2453318 and stream_source_uri only 1043871). This information will be important later on when we prepare the dataset for various ML tasks - we will need to deal with these Null values somehow. We also see that 4 columns (e.g. referral_code or stream_cached) have 0 values in them, which means that we will drop these columns, as they serve no purpose nor benefit.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at descriptive summary statistics of each column \n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Innitially this block was run at the very beginning, to decide on the empty columns which can be dropped\n",
    "# This block shows now that the columns keps are not empty and contain a fair amount of information\n",
    "for col in data.columns:\n",
    "    pct_missing = np.mean(data[col].isnull())\n",
    "    if pct_missing != 0:\n",
    "        print('{} - {}%'.format(col, round(pct_missing*100))) # look into financial product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 AGE, STREAM LENGTH, HOUR, WEEKDAY DISTRIBUTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING AGE, STREAM LENGTH, HOUR, WEEKDAY DISTRIBUTION  \n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "rs = np.random.RandomState(10)\n",
    "\n",
    "f, axes = plt.subplots(2, 2, figsize=(15, 7))\n",
    "sns.despine(left=True)\n",
    "\n",
    "sns.distplot(data['user_age'], bins=50, fit=norm, ax=axes[0, 0])\n",
    "sns.distplot(data[\"stream_length\"], bins=50,fit=norm, ax=axes[0, 1])\n",
    "sns.distplot(data[\"hour\"], bins=24,fit=norm, ax=axes[1, 0])\n",
    "sns.distplot(data[\"weekday\"], bins=7, fit=norm, ax=axes[1, 1])\n",
    "\n",
    "axes[0,0].set_xlim(10, 100)\n",
    "axes[0,1].set_xlim(0,500)\n",
    "\n",
    "axes[0,0].set_title(\"Age Distribution\")\n",
    "axes[0,1].set_title(\"Stream Lenght Distribution\")\n",
    "axes[1,0].set_title(\"Hour Distribution\")\n",
    "axes[1,1].set_title(\"Weekday Distribution\")\n",
    "\n",
    "axes[0,0].set_xlabel('')\n",
    "axes[0,1].set_xlabel('')\n",
    "axes[1,0].set_xlabel('')\n",
    "axes[1,1].set_xlabel('')\n",
    "\n",
    "plt.setp(axes, yticks=[])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <div class=\"alert alert-success\">\n",
    "<b> AGE, STREAM LENGTH, HOUR, WEEKDAY DISTRIBUTION </b>\n",
    "\n",
    "<p> The age distribution is heavily right-skewed with a mean around 29 and a mode of 24, which is to be expected as Spotify required users to be above 13 as a minimum, and young people are more adapt at adopting digital trends such as music streaming and smartphones. A long tail is observable, which leads to the right-skewedness of the distribution. </p>\n",
    "<p> Most streams are in the area of 150 – 250 seconds in this left skewed distribution. This is the intuitive duration for songs. The black curve represents a standard-normal distribution for comparison. </p> \n",
    "<p> Users stream more on Monday and Saturday, and mostly in the afternoon in the hours of 14:00 – 20:00. Broken down by our key playlists, we see a continuation of this pattern in the below graph. </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 USER-BASED FEATURES DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENDER SPLIT\n",
    "mpl.style.use('seaborn-white')\n",
    "\n",
    "labels = 'Male', 'Female'\n",
    "sizes = [sum(data[\"gender\"]!=\"female\"), sum(data[\"gender\"]==\"female\")]\n",
    "colors = [\"#1aa64b\",\"#363837\"]\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.title('Gender Distribution')\n",
    "plt.pie(sizes,  explode=(0.04,0), labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, pctdistance=1.2,labeldistance=1.4)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER-BASED FEATURES DISTRIBUTION\n",
    "f, axes = plt.subplots(4, 2, figsize=(20, 35))\n",
    "\n",
    "prob = data.stream_os.value_counts(normalize=True)\n",
    "threshold = 0.05\n",
    "mask = prob > threshold\n",
    "tail_prob = prob.loc[~mask].sum()\n",
    "prob = prob.loc[mask]\n",
    "prob['other'] = tail_prob\n",
    "prob.plot(kind='bar', color=[\"#1aa64b\",\"#363837\"], ax=axes[0, 0])\n",
    "axes[0,0].set_title(\"Operating System Distribution\")\n",
    "axes[0,0].set_xticklabels(prob.index, rotation=0)\n",
    "\n",
    "data_plot_1 = data.stream_device.value_counts(normalize=True)\n",
    "data_plot_1.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[0,1])\n",
    "axes[0,1].set_title(\"Device Distribution\")\n",
    "axes[0,1].set_xticklabels(data_plot_1.index, rotation=0)\n",
    "\n",
    "data_plot_2 = data.mobile.value_counts(normalize=True)\n",
    "data_plot_2.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[1,0])\n",
    "axes[1,0].set_title(\"Mobile Distribution\")\n",
    "axes[1,0].set_xticklabels(data_plot_2.index, rotation=0)\n",
    "\n",
    "data_plot_3 = data.partner_name.value_counts(normalize=True)\n",
    "data_plot_3.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[1,1])\n",
    "axes[1,1].set_title(\"Provider Distribution\")\n",
    "axes[1,1].set_xticklabels(data_plot_3.index, rotation=0)\n",
    "\n",
    "data_plot_4 = data.user_product_type.value_counts()\n",
    "data_plot_4.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[2,0])\n",
    "axes[2,0].set_title(\"User Product Type distribution\")\n",
    "axes[2,0].set_xticklabels(data_plot_4.index, rotation=0)\n",
    "\n",
    "data_plot_5 = data.stream_source.value_counts(normalize=True)\n",
    "data_plot_5.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[2,1])\n",
    "axes[2,1].set_title(\"Stream Source Distribution\")\n",
    "axes[2,1].set_xticklabels(data_plot_5.index, rotation=0)\n",
    "\n",
    "data_plot_6 = data.financial_product.value_counts(normalize=True)\n",
    "data_plot_6.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[3,0])\n",
    "axes[3,0].set_title(\"Product Distribution\")\n",
    "axes[3,0].set_xticklabels(data_plot_6.index, rotation=45)\n",
    "\n",
    "data_plot_7 = data.access.value_counts()\n",
    "data_plot_7.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[3,1])\n",
    "axes[3,1].set_title(\"User Subscription Type Distribution\")\n",
    "axes[3,1].set_xticklabels(data_plot_7.index, rotation=0)\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>User Demographics</b>\n",
    "\n",
    "<p>Our userbase contains slightly more females then males. The majority use an iOS mobile device, have either Vodafone or Boku as their mobile broadband provider and belong to the user category \"paid\". Most streaming is done from collections. Users mostly fall under the financial product category \"student\" and have a \"premium\" type subscription. Such subscription profile corresponds to the age profile of Spotify users. This is also potentially an evidence that users are likely to be price sensitive. However, it can also be seen that there is still space to increase revenue. </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 TIME SERIES PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME SERIES PLOT\n",
    "\n",
    "# Create a DataFrame of the total number of streams per date  \n",
    "df_date = data['date'].value_counts()\n",
    "df_date=pd.DataFrame(df_date.sort_index())\n",
    "df_date.reset_index(level=df_date.index.names, inplace=True) \n",
    "df_date=df_date.rename(columns={\"index\": \"Date\"})\n",
    "df_date=df_date.rename(columns={\"date\":\"Number Of Streams\"}) \n",
    "\n",
    "# Create the bar graph to better visualize time series\n",
    "sns.set(style=\"whitegrid\")\n",
    "f, ax = plt.subplots(figsize=(6, 15))\n",
    "sns.barplot(x=(df_date['Number Of Streams']), y=df_date['Date'],\n",
    "            data=df_date,            \n",
    "            label=\"Number of streams by month\",\n",
    "            orient = \"h\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Time series distribution of streams </b>\n",
    "\n",
    "<p> We notice that the number of streams follows an upward trend. This indicates that Spotify, as expected, has raised in demand from year to year, seeing a significant increase in the last few years. However, there are some declines at certain dates, which shows that there might be a  degree of seasonality in the time series.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 ARTISTS AND PLAYLISTS PLOTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next up, we will show how often various playlists come up in the data\n",
    "# Because we have so many playlists, we only display those who make up more than 2 % of the total data\n",
    "# https://stackoverflow.com/questions/37598665/how-to-plot-a-value-counts-in-pandas-that-has-a-huge-number-of-different-counts\n",
    "\n",
    "f, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "prob = data.playlist_name.value_counts(normalize=True)\n",
    "threshold = 0.02\n",
    "mask = prob > threshold\n",
    "tail_prob = prob.loc[~mask].sum()\n",
    "prob = prob.loc[mask]\n",
    "prob['other'] = tail_prob\n",
    "prob.plot(kind='bar', color=[\"#1aa64b\",\"#363837\"], ax=axes[0, 0])\n",
    "axes[0,0].set_title(\"Playlists Distribution\")\n",
    "\n",
    "# But here we show all artists who make up more than 5 % of total streams - 5 % would lead to an overcrowded graph\n",
    "prob = data.artist_name.value_counts(normalize=True)\n",
    "threshold = 0.05\n",
    "mask = prob > threshold\n",
    "tail_prob = prob.loc[~mask].sum()\n",
    "prob = prob.loc[mask]\n",
    "prob['other'] = tail_prob\n",
    "prob.plot(kind='bar', color=[\"#1aa64b\",\"#363837\"], ax=axes[0, 1])\n",
    "axes[0,1].set_title(\"Artists Distribution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Artist and Plylist distribution</b>\n",
    "\n",
    "<p>The Playlists suggested by the Warner Analysts (Hot Hits UK, Massive Dance Hits, The Indie List, New Music Friday) are not very popular, with the exception of Hot Hits UK, which indeed is the very most popular playlist. The others do not show up at all with the threshold of 2%, however, we see that the majority of playlists are small ones.</p> \n",
    "\n",
    "<p>For artists, we see that only 6 artists have more than 5 % of total streams each, with Charlie Puth being the \"Most popular artist\" by far, followed by Dua Lipa and Lukas Graham.</p> \n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP 10 MOST STREAMED SUCCESSFUL ARTISTS BAR CHART \n",
    "\n",
    "mpl.style.use('seaborn-white')\n",
    "top_artists_df = data.loc[data['success'] == 1]['artist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "objects = top_artists_df.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = top_artists_df.values\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.title('Top 10 most streamed successful artists')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Top 10 most streamed successful artists distribution</b>\n",
    "\n",
    "<p>The most streamed successful artist is Charlie Puth by far, followed by Dua Lipa and Lukas Graham.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP PLAYLSIST WHERE SUCCESSFUL ARTISTS SHOW UP\n",
    "\n",
    "# stronger recommendation based on other playlists - key playlists may not be relevant any more, many other playlists have way more streams\n",
    "top_unpopular_playlists =  successful_artists_df.loc[successful_artists_df['success']==1]['playlist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "objects = top_unpopular_playlists.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = top_unpopular_playlists.values\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.title('Top Playlists of Successful Artists')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYLISTS CONTAINING THE HIGHEST NUMBER OF DISTINCT SUCCESSFUL ARTISTS! --> HHUK stands out\n",
    "\n",
    "successful_artists_frequency_playlists = successful_artists_df.groupby('playlist_name')['artist_name'].nunique().sort_values(ascending=False).reset_index(name='successful_artists')[:15]\n",
    "objects = successful_artists_frequency_playlists.playlist_name\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = successful_artists_frequency_playlists.successful_artists\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Number of Unique Successful Artists')\n",
    "plt.title('Playlists which contain the highest number of distinct successful artists')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Playlists with successful artists</b>\n",
    "\n",
    "<p> Upon looking at which playlists artists who have been flagged successful feature on, it becomes manifests the insight that Hot Hits UK has by far the most successful artists. However, the other 3 playlists do not show up at all. It must be questioned whether the other 3 playlists are a good indicator of success or if the key playlists should be updated. </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP 10 PLAYLISTS MOST ARTISTS SHOW UP, EXCLUSING THE 4 \"SUCCESS\" RELATED ONES\n",
    "\n",
    "successful_artists_df = data.loc[data['artist_name'].isin(successful_artists)]\n",
    "print('Total successful artists:', successful_artists_df['artist_name'].nunique())\n",
    "top_unpopular_playlists = successful_artists_df.loc[successful_artists_df['successful']==False]['playlist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "objects = top_unpopular_playlists.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = top_unpopular_playlists.values\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.title('Top 10 playlists most popular artists show up, different than \"The 4\"')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Playlists with successful artists - excluding the key playlists</b>\n",
    "\n",
    "<p> 'Today's Top Hits' is the playlist with most successful artists when we exclude the key playlists. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6  KEY PLAYLISTS VIZUALIZATIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Hot Hits UK\", \"Massive Dance Hits\", \"The Indie List\", \"New Music Friday\"\n",
    "print('From a total of {} successful artists'.format(len(successful_artists)))\n",
    "print('Total successful artists showing up on Hot Hits UK:', successful_artists_df.loc[successful_artists_df['playlist_name'] == 'Hot Hits UK']['artist_name'].nunique())\n",
    "print('Total successful artists showing up on Massive Dance Hits:', successful_artists_df.loc[successful_artists_df['playlist_name'] == 'Massive Dance Hits']['artist_name'].nunique())\n",
    "print('Total successful artists showing up on The Indie List:', successful_artists_df.loc[successful_artists_df['playlist_name'] == 'The Indie List']['artist_name'].nunique())\n",
    "print('Total successful artists showing up on New Music Friday:', successful_artists_df.loc[successful_artists_df['playlist_name'] == 'New Music Friday']['artist_name'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUTION OF STREAMS PER HOUR FOR THE KEY PLAYLISTS - Line chart showing that HHUK is signifficantly more streamed than the others\n",
    "\n",
    "# 4 key Playlists \n",
    "key_playlists = playlist_mapper[playlist_mapper[\"name\"].isin([\"Hot Hits UK\", \"Massive Dance Hits\", \"The Indie List\", \"New Music Friday\"])]\n",
    "data_key_playlists = data[data.stream_source_uri.astype(str).str[-22:].isin(key_playlists.id)]\n",
    "\n",
    "# Normalize to identify hourly trends \n",
    "key_playlists_streams_hour = data_key_playlists.groupby(['playlist_name','hour']).size().unstack().fillna(0).transpose()\n",
    "key_playlists_streams_hour.plot(kind='line',figsize=(12,6))\n",
    "\n",
    "# Visulaize the distribution of streams per hour (Normalized)\n",
    "key_playlists_streams_hour_norm = pd.DataFrame(scale(key_playlists_streams_hour))\n",
    "key_playlists_streams_hour_norm.columns = key_playlists_streams_hour.columns\n",
    "key_playlists_streams_hour_norm.plot(title='Distribution Of Streams Per Hour (Normalized)',kind='line',figsize=(12,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Distribution of streams per hour for the key playlists </b>\n",
    "\n",
    "<p> We observe that overall users tend to stream the key playlists mostly from 10 a.m. to 3 p.m. From 5 a.m. to 10 a.m the number of streams start to raise while from 3 p.m to 8 p.m the  number of streams falls. During the late hours of the night there are not much streams. This is quite expected if we think of a normal person's day-to-day rutine.</p> \n",
    "<p> We should also look at each key playlist's hourly trend. The Hot Hits UK playlist has the biggest number of streams somewhere between 3 p.m. and 8 p.m., while it sees a small decrease in the number of streams from 10 a.m to 3 p.m. Other than that, it follows the overall hourly trend discussed before. The Massive Dance Hits playlist seems to count the biggest number of streams at 3 p.m. and it generally follows the overall hourly trend discussed before. The New Music Friday playlist seems to also count the biggest number of streams at 3 p.m. We also observe an increase in the number of streams after 8 p.m. which does not happen for the other key playlists. Other than that, it tends to follow the overall hourly trend discussed before, but it has (most) signifficant ups and downs in number of streams. The Indie list playlists seems to count the biggest number of streams at 10 a.m. Other than that, it follows the overall hourly trend discussed before.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MATRIX WITH ARTISTS DISTRIBUTION ACROSS TOP PLAYLISTS  \n",
    "\n",
    "playlists_filtered = data.stream_source_uri.value_counts().head(20).keys().tolist()\n",
    "artists_filtered = data.artist_name.value_counts().head(20).keys().tolist()\n",
    "df = data[:]\n",
    "df['playlist_name'] = df.stream_source_uri.astype(str).str[-22:].map(playlist_mapper.set_index('id')['name'])\n",
    "df = df.dropna(subset=['playlist_name','artist_name'])\n",
    "df = df[(df.stream_source_uri.isin(playlists_filtered))&df.artist_name.isin(artists_filtered)]\n",
    "df = df.groupby(['artist_name','playlist_name']).size().unstack().fillna(0)\n",
    "plot_cluster(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MATRIX WITH ARTISTS DISTRIBUTION ACROSS TOP PLAYLISTS EXCLUDING HHUK \n",
    "\n",
    "# CREATE A NEW DATAFRAME WHICH EXCLUDES HHUK\n",
    "successful_artists_df_no_HHUK = successful_artists_df.loc[successful_artists_df.playlist_name != 'Hot Hits UK']\n",
    "successful_artists_df_no_HHUK.artist_name.nunique()\n",
    "\n",
    "filter_playlists = successful_artists_df_no_HHUK.stream_source_uri.value_counts().head(20).keys().tolist()\n",
    "filter_artists = successful_artists_df_no_HHUK.artist_name.value_counts().head(20).keys().tolist()\n",
    "t = successful_artists_df_no_HHUK[:]\n",
    "t['playlist_name'] = t.stream_source_uri.astype(str).str[-22:].map(playlist_mapper.set_index('id')['name'])\n",
    "t = t.dropna(subset=['playlist_name','artist_name'])\n",
    "t = t[(t.stream_source_uri.isin(filter_playlists))&t.artist_name.isin(filter_artists)]\n",
    "t = t.groupby(['artist_name','playlist_name']).size().unstack().fillna(0)\n",
    "\n",
    "plot_cluster(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Cluster the playlists and artists by number of streams</b>\n",
    "\n",
    "<p> The above two heatmaps show the difference when the dominant Hot Hits UK playlist is removed from the analysis – while the artists stay mostly the same, the distribution across the other playlists becomes much clearer. A separation into two distinct success categories, one without Hot Hits UK, is a worthwhile experiment. </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Artist Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Artist Feature 1 -> TOTAL NUMBER OF STREAMS BY ARTIST\n",
    "def artist_total_streams(data):\n",
    "    ar_stream_df = data['artist_name'].value_counts().to_frame().reset_index()\n",
    "    ar_stream_df = ar_stream_df.rename(columns={\"index\" : \"artist_name\", \"artist_name\" : \"ar_stream_count\"})\n",
    "    return ar_stream_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## graph\n",
    "# call funtion and generate the temp feature for viz\n",
    "# code for viz\n",
    "'''\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "'''\n",
    "# what we can see after this function.....\n",
    "\n",
    "## each feature the same a.m. a. p.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Artist Feature 2 -> TOTAL NUMBER OF USERS BY ARTIST\n",
    "def artist_total_users(data):\n",
    "    ar_users_df = data.groupby('artist_name')['customer_id'].nunique().sort_values(ascending=False).reset_index(name='ar_users')\n",
    "    return ar_users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Artist Feature 3 -> PASSION SCORE BY ARTIST\n",
    "def artist_passion_score(data):\n",
    "    ar_passion_df = artist_total_streams(data)\n",
    "    ar_passion_df = ar_passion_df.merge(artist_total_users(data), on='artist_name', how='left', sort=False)\n",
    "    ar_passion_df['ar_passion_score'] = ar_passion_df['ar_stream_count']/ar_passion_df['ar_users']\n",
    "    ar_passion_df.drop(columns=['ar_stream_count','ar_users'], inplace=True)\n",
    "    return ar_passion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Artist Feature 4 -> GENRE PCA BY ARTIST ** NEW FEATURE **\n",
    "def artist_genres_with_pca(data, no_components_or_preserved_variance):\n",
    "    artists_list = get_list_unique_artists(data)\n",
    "    artist_names = []\n",
    "    artist_ids = []\n",
    "    artist_genres = []\n",
    "\n",
    "    for i in artists_list:\n",
    "        try:\n",
    "            result = sp.search(i) #search query\n",
    "            artist_ids.append(result['tracks']['items'][0]['artists'][0]['id'])\n",
    "            artist_names.append(i)\n",
    "\n",
    "            result = sp.artist(result['tracks']['items'][0]['artists'][0]['uri'])\n",
    "            artist_genres.append(result['genres'])\n",
    "        except IndexError as error:\n",
    "            print(i) \n",
    "\n",
    "    temp_df = pd.DataFrame(zip(artist_names, artist_genres), columns=[\"artist_name\", \"genres\"]) \n",
    "    final_list = []\n",
    "\n",
    "    col_one_list = temp_df['genres'].tolist()\n",
    "    flat_list = [item for sublist in col_one_list for item in sublist]\n",
    "    mylist = list(dict.fromkeys(flat_list))\n",
    "\n",
    "    new_temp_df = pd.DataFrame(columns=[mylist], index=[temp_df[\"artist_name\"]]) \n",
    "\n",
    "    expanded_dataframe = expand_list(temp_df,\"genres\", \"genres\")\n",
    "    expanded_dataframe['value']=1\n",
    "\n",
    "    pvit = expanded_dataframe.pivot(index='artist_name', columns='genres', values='value')\n",
    "    pvit.fillna(0, inplace=True)\n",
    "    pvit.reset_index(inplace=True)\n",
    "    \n",
    "    # Run PCA on Components Saving 40 columns\n",
    "    Y = pvit['artist_name']\n",
    "    X = pvit.iloc[:,1:]\n",
    "    \n",
    "    # Keeping 40 Principal Components which preserve rougly 50% of the variance\n",
    "    n_components = no_components_or_preserved_variance\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    \n",
    "    n_components_column_names = []\n",
    "\n",
    "    for i in range(1,n_components+1):\n",
    "        n_components_column_names.append('genre_pc_' + str(i))\n",
    "    \n",
    "    principalComponents = pd.DataFrame(data = principalComponents, columns =n_components_column_names)\n",
    "    principalComponents = pd.concat([Y, principalComponents],axis=1)\n",
    "    \n",
    "    return principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## graphs for pca\n",
    "# code for viz\n",
    "'''\n",
    "distribution\n",
    "feature importance \n",
    "variance explained\n",
    "...\n",
    "\n",
    "'''\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Artist Feature 5 -> Region PCA BY ARTIST ** OLD PCA Analysis Rolled into a function **\n",
    "def artist_regions_with_pca(data, no_components_or_preserved_variance):\n",
    "    \n",
    "    artist_region = data.groupby(['artist_name','region_code']).agg({'region_code': ['count']})\n",
    "\n",
    "    feature_region = artist_region.unstack().fillna(0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    artist_region_scaled = scaler.fit_transform(feature_region)\n",
    "\n",
    "    pca = decomposition.PCA(no_components_or_preserved_variance)\n",
    "\n",
    "    artist_region_pca = pca.fit(artist_region_scaled)\n",
    "\n",
    "    artist_region_pca_features = pca.fit_transform(artist_region_scaled)\n",
    "\n",
    "    pc_name=[]\n",
    "    for i in range(1,11):\n",
    "        pc_name.append('region_pc_'+str(i))\n",
    "\n",
    "    region_pcaDF = pd.DataFrame(data=np.transpose(artist_region_pca.components_), columns=pc_name)\n",
    "    artist_pca = pd.Series(feature_region.index.values, name='artist_name')\n",
    "\n",
    "    region_pcaDF = pd.concat([artist_pca, region_pcaDF],axis=1)\n",
    "\n",
    "    return region_pcaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## graphs for pca\n",
    "# code for viz\n",
    "'''\n",
    "distribution\n",
    "feature importance \n",
    "variance explained\n",
    "...\n",
    "\n",
    "'''\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Artist Feature 6 -> Region PCA BY ARTIST ** OLD PCA Analysis Rolled into a function **\n",
    "def artist_avg_track_streaming_time(data):\n",
    "    ar_avg_track_streaming_df\n",
    "    return ar_avg_track_streaming_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Playlist Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Playlist Feature 1 -> TOTAL NUMBER OF STREAMS ON TOP 20 PLAYLISTS BY ARTIST\n",
    "def playlist_top20_total_streams(data):\n",
    "    pasc = data.groupby('playlist_name')['playlist_name'].count()\n",
    "    df_playlist = pasc.to_frame().rename(columns={'playlist_name': 'pl_stream_count'})\n",
    "    df_playlist.reset_index(inplace=True)\n",
    "    df_ar_pl = data.groupby(['artist_name','playlist_name']).size()\n",
    "    df_ar_pl = df_ar_pl.groupby(level=0, group_keys=False).nlargest(20)\n",
    "    df_ar_p1_reset = df_ar_pl.to_frame().reset_index()\n",
    "    df_ar_p1_reset.rename(columns={0:'count'}, inplace=True)\n",
    "    df_ar_p1_merge = df_ar_p1_reset.merge(df_playlist, on='playlist_name', how='inner')\n",
    "    df_ar_p1_merge = df_ar_p1_merge.sort_values(by=['artist_name', 'count'], ascending=[1,0])\n",
    "    df_ar_p1_merge.drop(columns=['pl_stream_count'], inplace=True)\n",
    "    artist_sum = df_ar_p1_merge.groupby(['artist_name']).agg({'count':sum})\n",
    "    df_ar_p1_merge = df_ar_p1_merge.merge(artist_sum,on='artist_name', how='inner', suffixes=('','_sum'))\n",
    "    df_ar_p1_merge.drop(columns=['playlist_name','count'], inplace=True)\n",
    "    df_ar_p1_merge.rename(columns={\"count_sum\" : \"pl_total_streams_top20\"}, inplace=True)\n",
    "    df_ar_p1_merge.drop_duplicates(inplace=True)\n",
    "    df_ar_p1_merge.reset_index(drop=True, inplace=True)\n",
    "    return df_ar_p1_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Playlist Feature 2 ->  TOTAL NUMBER OF UNIQUE USERS ON TOP 20 PLAYLISTS BY ARTIST\n",
    "def playlist_top20_user_counts(data):\n",
    "    df_temp = pd.DataFrame(columns=['artist_name', 'total_users_top20playlists'])\n",
    "\n",
    "    for i in list_uniqueartists:\n",
    "        df_artist = df_withoutkeyplaylists[df_withoutkeyplaylists['artist_name']==i]\n",
    "        df_artist = pd.DataFrame(df_artist.groupby(['playlist_name'])['customer_id'].nunique())\n",
    "        df_artist.sort_values(by='customer_id', ascending=False, inplace=True)\n",
    "        df_artist = df_artist.head(20)\n",
    "        df_artist.insert(0, 'artist_name',i)\n",
    "        df_artist.reset_index(inplace=True)\n",
    "        df_artist.drop(columns='playlist_name', inplace=True)\n",
    "        df_artist = pd.DataFrame(df_artist.groupby('artist_name')['customer_id'].sum())\n",
    "        df_artist.rename(columns={'customer_id':'total_users_top20playlists'},inplace=True)\n",
    "        df_temp = pd.concat([df_temp, df_artist], axis=0)\n",
    "\n",
    "    df_temp.reset_index(inplace=True)\n",
    "    df_temp.drop(columns='artist_name', inplace=True)\n",
    "    df_temp.rename(columns={'index':'artist_name'},inplace=True)\n",
    "\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Playlist Feature 3 ->  PASSION SCORE ON TOP 20 PLAYLISTS BY ARTIST\n",
    "def playlist_top20_passion_score(data):\n",
    "    temp_df = playlist_top20_total_streams(data)\n",
    "    pl_passion_df = temp_df.merge(playlist_top20_user_counts(data), on='artist_name', how='left', sort=False)\n",
    "    pl_passion_df['pl_passion_score_top20'] = pl_passion_df['pl_total_streams_top20']/pl_passion_df['total_users_top20playlists']\n",
    "    pl_passion_df.drop(columns=['pl_total_streams_top20','total_users_top20playlists'], inplace=True)\n",
    "    return pl_passion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - Playlist Feature 4 -> AVERAGE TRACK STREAMING TIME BY PLAYLIST ** NEW FEATURE **\n",
    "def playlist_average_track_streaming_time(data):\n",
    "    avg_streamed_tracks_per_playlist = data.groupby(['playlist_name', 'track_name'])['log_time'].nunique().groupby(['playlist_name']).mean().sort_values(ascending = False).to_frame().reset_index().rename(columns={\"log_time\" : \"avg_tracks_streaming\"})\n",
    "    return avg_streamed_tracks_per_playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 User Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - User Feature 1 -> TOTAL FEMALE USERS BY ARTIST\n",
    "def user_total_female(data):\n",
    "    df_genderbreakdownbyartist = data[['artist_name','gender']].copy()\n",
    "    df_genderbreakdownbyartist['female']=0\n",
    "    df_genderbreakdownbyartist.loc[df_genderbreakdownbyartist['gender'] == 'female', 'female'] = 1\n",
    "    df_genderbreakdownbyartist = df_genderbreakdownbyartist.groupby(['artist_name']).agg({'female':'sum'})\n",
    "    df_genderbreakdownbyartist.sort_values(by='female', ascending=False, inplace=True)\n",
    "    return df_genderbreakdownbyartist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZ - User Feature 2 -> BREAKDOWN OF USER AGES BY ARTIST\n",
    "def user_age_breakdown(data):\n",
    "    \n",
    "    df_temp = data\n",
    "\n",
    "    bins = [0, 18, 30, 40, 50, 60, 70, 200]\n",
    "    labels = ['<18','18-29', '30-39', '40-49', '50-59', '60-69', '70<']\n",
    "    df_temp['user_age_range'] = pd.cut(df_temp.user_age, bins, labels = labels,include_lowest = True)\n",
    "\n",
    "    df_temp = pd.concat([df_temp['artist_name'], df_temp['user_age_range']], axis=1)\n",
    "    df_temp['count_dummy'] = 1\n",
    "    df_temp = pd.pivot_table(df_temp, values='count_dummy', index=['artist_name'],\n",
    "                          columns=['user_age_range'], aggfunc=np.sum)\n",
    "\n",
    "    df_temp.fillna(0,inplace=True)\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 region feature (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Merge the features into the analytics-ready DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurefunctions = [artist_total_streams(data), \n",
    "                    artist_total_users(data),\n",
    "                    artist_passion_score(data),\n",
    "                    artist_genres_with_pca(data, 40), # This one in particular takes ages as it needs to connect to the Spotify API\n",
    "                    artist_regions_with_pca(data, 10),\n",
    "                    playlist_top20_total_streams(data),\n",
    "                    playlist_top20_user_counts(data),\n",
    "                    playlist_top20_passion_score(data),\n",
    "#                     playlist_average_track_streaming_time(data), TBC \n",
    "                    user_total_female(data),\n",
    "                    user_age_breakdown(data)\n",
    "                   ]\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit takes a while, sit back, chill, have a coffee\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for i in featurefunctions:\n",
    "    if final_df.empty:\n",
    "        final_df = i\n",
    "    else: \n",
    "        final_df = final_df.merge(i, on='artist_name', how='left', sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df = final_df.set_index('artist_name')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Features Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_temp = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = final_df_temp.corr()\n",
    "plt.figure(figsize=(16, 10))\n",
    "heatmap = sns.heatmap(correlation >0.95 ,  linewidths=0, vmin=-0.75, vmax=1, cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPPINGG THE COLUMNS THAT CORRELATE\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = final_df_temp.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.75\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.50)]\n",
    "\n",
    "#final_df_temp = final_df_temp.drop(columns=to_drop)\n",
    "\n",
    "print(\"Correlated features dropped: \")\n",
    "print(*to_drop, sep = \", \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_temp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the ratio of successful artist to unsuccessful artists in the dataset is 1:5. This represents a fairly major class-imbalance. \n",
    "In general class imbalances can lead to strong predictive biases, as models can optimize the training objective by learning to predict the majority label most of the time. Furthermore, over-sampling can lead to over-fitting of the smaller class. (Source: Python Papaer - ask Ioana).\n",
    "\n",
    "Therefore, we will adopt random undersampling technique, which will get the proportions of classes closer to each other. We shall under-sample the unsuccessful artist enough to balance the classes and not lose too much information. A 60:40 ratio should be apppropriate. This new class imbalance proportion will be less significant for the modesl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count = final_df_drop.success.value_counts()\n",
    "print('failure :', success_count[0])\n",
    "print('success :', success_count[1])\n",
    "print('Proportion:', round(success_count[0] / success_count[1], 2), ': 1')\n",
    "\n",
    "success_count.plot(kind='bar', title='Count (success)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_fail, count_suc  = final_df_drop.success.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_suc = final_df_drop[final_df_drop['success'] == 1]\n",
    "df_fail = final_df_drop[final_df_drop['success'] == 0]\n",
    "print(df_suc.shape)\n",
    "print(df_fail.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random under-sampling\n",
    "df_fail_under = df_fail.sample(int(count_suc/40*60))  # achive a class balance closer to 60-40.\n",
    "\n",
    "df_class_balance = pd.concat([df_fail_under, df_suc], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_class_balance.success.value_counts())\n",
    "\n",
    "df_class_balance.success.value_counts().plot(kind='bar', title='Count (success)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Features Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Generation (Classical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Logistic Regression with Grid Search Tuning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lr_clf_grd = LogisticRegression(random_state=42, solver = 'liblinear')\n",
    "params = {'penalty': ('l1', 'l2', 'none'),\n",
    "          'max_iter': [10,100,1000,10000],\n",
    "          'tol': [1e-4, 1e-3, 1e-2, 1e-1, 1], \n",
    "          'C':[0.001,.009,0.01,.09,1,5,10,25] \n",
    "         }\n",
    "best_lr_clf = GridSearchCV(lr_clf_grd, param_grid=params, cv=5, scoring = 'recall')\n",
    "best_lr_clf.fit(X_train,y_train)\n",
    "print(best_lr_clf.best_params_)\n",
    "print(best_lr_clf.best_score_)\n",
    "\n",
    "lr_clf_optimized = LogisticRegression(random_state=42, solver = 'liblinear', **best_lr_clf.best_params_)\n",
    "lr_clf_optimized.fit(X_train, y_train)\n",
    "y_pred_test_lr = lr_clf_optimized.predict(X_test)\n",
    "\n",
    "print(\"MSE Optimised Logistic Regression test error:\", mean_squared_error(y_test, y_pred_test_lr))\n",
    "print(\"Accuracy score:\", lr_clf.score(X_test, y_test)) # accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on the strong(naive) assumption, that every feature is independent of the others, in order to predict the category of a given sample. It learns p(y|x) indirectly by infering p(x/y) and p(y) first. \n",
    "\n",
    "As it takes only 2 parameters, and is strictly based on probabilities and likelyhood, performing Grid Search does not make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test_nb = nb_clf.predict(X_test)\n",
    "print(classification_report(y_test, nb_predict, target_names = label_list))\n",
    "\n",
    "print(\"MSE of Naive Bayes test:\", mean_squared_error(y_test, y_pred_test_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Support Vector Machine\n",
    "\n",
    "Feature importance works only with linear kernel. For other kernels it is not possible because data are transformed by kernel method to another space, which is not related to input space. Weights asigned to the features (coefficients in the primal problem). This is only available in the case of linear kernel. Its coefficients can be viewed as weights of the input's \"dimensions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with Grid Search Tuning\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "params = {\n",
    "    'kernel' : ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "    'degree' : [1, 2, 3, 4, 5],\n",
    "    'tol': [1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "    'probability': (True, False)\n",
    "         }\n",
    "\n",
    "svc_grid_clf = SVC(random_state=42)\n",
    "best_svm_clf = GridSearchCV(svc_grid_clf, param_grid=params, cv=5, scoring = 'recall')\n",
    "best_svm_clf.fit(X_train,y_train)\n",
    "print(best_svm_clf.best_params_)\n",
    "print(best_svm_clf.best_score_)\n",
    "\n",
    "svm_clf_optimized = SVC(random_state=42, **best_svm_clf.best_params_)\n",
    "svm_clf_optimized.fit(X_train, y_train)\n",
    "y_pred_test_svm = svm_clf_optimized.predict(X_test)\n",
    "\n",
    "print(\"MSE of SVC test:\", mean_squared_error(y_test, y_pred_test_svm))\n",
    "print(\"Accuracy score:\", svm_clf.score(X_test, y_test)) # accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Decision Tree\n",
    "\n",
    "Gini scoring is used instead of Entropy because:\n",
    "\n",
    "* Performance will not change whether you use Gini impurity or Entropy\n",
    "* Entropy might be a little slower to compute (because it makes use of the logarithm).\n",
    "* It only matters in 2% of the cases whether you use gini impurity or entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# SVM with Grid Search Tuning\n",
    "\n",
    "params = {\n",
    "    'splitter': ('best', 'random'),\n",
    "    'max_depth': [2, 3, 4, 5, 6],\n",
    "    'min_samples_split': range(10,500,20),\n",
    "    'min_samples_leaf': [1, 2, 3, 4],\n",
    "    'max_features': ('int', 'float', 'auto', 'sqrt', 'log2', None)\n",
    "         }\n",
    "\n",
    "tree_grid_clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "best_tree_clf= GridSearchCV(tree_grid_clf, param_grid=params, cv=5, scoring = 'recall')\n",
    "best_tree_clf.fit(X_train,y_train)\n",
    "print(best_tree_clf.best_params_)\n",
    "print(best_tree_clf.best_score_)\n",
    "\n",
    "tree_clf_optimized = DecisionTreeClassifier(**best_tree_clf.best_params_)\n",
    "tree_clf_optimized.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test_tree = tree_clf_optimized.predict(X_test)\n",
    "\n",
    "# Probabilities for each class\n",
    "y_tree_clf_probs = tree_clf_optimized.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"MSE of Decision Tree test:\", mean_squared_error(y_test, y_pred_test_tree))\n",
    "print(\"Accuracy score:\", tree_clf.score(X_test, y_test)) # accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Stochasting Gradient Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "SGD_clf=SGDClassifier()\n",
    "SGD_clf.fit(X_train, y_train)\n",
    "\n",
    "# produce prediction\n",
    "y_pred_test_SGD = SGD_clf.predict(X_test)\n",
    "\n",
    "print(\"MSE of SGD:\", mean_squared_error(y_test, y_pred_test_SGD))\n",
    "print(\"Accuracy score:\", SGD_clf.score(X_test, y_test)) # accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 KNNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run KNN on multiple n vales to understand the value that optimises for Training and Test\n",
    "\n",
    "neighbors = list(range(1,30))\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for n in neighbors:\n",
    "    model = KNeighborsClassifier(n_neighbors=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_pred = model.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_train, train_pred)\n",
    "    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "    train_results.append(roc_auc)\n",
    "    y_pred = model.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "    test_results.append(roc_auc)\n",
    "    \n",
    "line1, = plt.plot(neighbors, train_results, 'b', label=\"Train AUC\")\n",
    "line2, = plt.plot(neighbors, test_results, 'r', label=\"Test AUC\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on visual interpretation of the AUC graph we chose n = 5\n",
    "#KKN_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "KKN_clf.fit(X_train, y_train)\n",
    "y_KKN_clf = KKN_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST WITH PARAMETERS TUNED\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [10, 50, 100, 200, 500],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [1, 2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3, 4],\n",
    "    'max_features': ('int', 'float', 'string', None)\n",
    "         }\n",
    "\n",
    "rnd_grid_clf = RandomForestClassifier(random_state=42)\n",
    "best_rf_clf = GridSearchCV(rnd_grid_clf, param_grid=params, cv=5, scoring = 'recall')\n",
    "best_rf_clf.fit(X_train,y_train)\n",
    "print(best_rf_clf.best_params_)\n",
    "print(best_rf_clf.best_score_)\n",
    "\n",
    "rnd_clf_otimized = RandomForestClassifier(random_state=42, **best_rf_clf.best_params_)\n",
    "rnd_clf_otimized.fit(X_train, y_train)\n",
    "y_pred_test_rf = rnd_clf_otimized.predict(X_test)\n",
    "\n",
    "print(\"MSE of Optimized Random Forest test:\", mean_squared_error(y_test, y_pred_test_rf))\n",
    "\n",
    "# Probabilities for each class\n",
    "y_random_forest_clf_prob = rnd_clf_otimized.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2 Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "y_adaboost_clf_pred = adaboost_clf.predict(X_test)\n",
    "\n",
    "print(\"MSE of Adaboost test:\", mean_squared_error(y_test, y_adaboost_clf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.3 Soft Voting Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#TBD\n",
    "estimators=[('Linear Regression', lr_clf_optimized), \n",
    "            ('Naive Bayes', nb_clf), \n",
    "            ('SVM', svm_clf_optimized), \n",
    "            ('KNN', KKN_clf), \n",
    "            ('Decision Tree', tree_clf_optimized), \n",
    "            ('SGD', SGD_clf)]\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test_voting = voting_clf.predict(X_test)\n",
    "\n",
    "print(\"MSE of Voting test:\", mean_squared_error(y_test, y_pred_test_voting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Extra Models (Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Knowledge Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Model Evaluation and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Confusion Matrix, Accuracy, Recall, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SUBPLOTS\n",
    "f, axes = plt.subplots(3, 3, figsize=(16, 16))\n",
    "\n",
    "countt = 1\n",
    "for name, clf in estimators:\n",
    "    ax = fig.add_subplot(3,3,countt)\n",
    "    y_predicted = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predicted) \n",
    "    make_confusion_matrix(cm)\n",
    "    ax.imshow(wordcloud, interpolation=\"bilinear\") # not sure if it works\n",
    "    ax.set_title(name)\n",
    "    countt+=1\n",
    "  #  ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CROSS VALIDATION SCORES ACROSS ALL MODELS\n",
    "model_names = []\n",
    "scores = []\n",
    "for name, clf in estimators:\n",
    "    score = cross_val_score(clf, x_train, y_train, cv=folds, verbose=0, scoring = 'recall')\n",
    "    # clf.fit(x_train, y_train)\n",
    "    # score = clf_k.score(x_test.values, y_test.values)\n",
    "    model_names.append(name)\n",
    "    scores.append(score.mean())\n",
    "    print(name + ': ' + str(score.mean()))\n",
    "\n",
    "performances = np.array(scores, dtype=np.float32).round(3)  \n",
    "plt.barh(model_names, performances, align='edge', alpha=0.6, color='deepskyblue')\n",
    "plt.xlabel('cross-validation score')\n",
    "plt.title('comparison of different classifiers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT CROSS VALIDATION rerall ACROSS ALL MODELS\n",
    "## graph\n",
    "# code for viz\n",
    "'''\n",
    "recall\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Feature-Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE IMPORTANCE FOR LOGISTIC REGRESSION\n",
    "rfe = RFE(lr_clf_optimized, n_features_to_select=10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "mask = rfe.support_\n",
    "print('Top 10 Most Important Features:')\n",
    "print(X_train_1.T[mask == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## graph\n",
    "# code for viz\n",
    "'''\n",
    "for loop all mode:\n",
    "    plot ROC curve for each model\n",
    "\n",
    "'''\n",
    "# what we can see after this function....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "************* some requirements **************\n",
    "'''\n",
    "1. all graphs consistent\n",
    "2. have titles\n",
    "3. similar color palette　(green and black)\n",
    "4. ..\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
