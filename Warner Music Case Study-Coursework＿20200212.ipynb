{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <br style =\"font-family:UCL-SoM-Outline;color:#EA7600\"> GROUP COURSEWORK </br> \n",
    "    </p>\n",
    "</h1>\n",
    "\n",
    "<div class=\"image\">\n",
    "\n",
    "<img src=\"./figure/som_ft.png\" width=\"70%\"  align=\"right\">\n",
    "<h4>\n",
    "          <p style=\"font-size:18pt\">MSIN0097 Predictive Analytics</p>\n",
    "          <p style=\"font-size:18pt;font-family:UCL-SoM-Solid; color:#EA7600;\">Student Name: <u>Jiarui Xin</u> </p> \n",
    "          <p style=\"font-size:18pt;font-family:UCL-SoM-Solid; color:#EA7600;\">ID: <u>19058391</u> </p> \n",
    "          <p style=\"font-size:18pt;font-family:UCL-SoM-Solid; color:#EA7600;\">Email: <u>jiarui.xin.19@ucl.ac.uk</u> </p> \n",
    "\n",
    "</h4>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSEWORK: WARNER MUSIC\n",
    "\n",
    "\n",
    "\n",
    "### PREDICTING THE SUCCESS OF ARTISTS ON SPOTIFY\n",
    "\n",
    "Please complete the sections of this Notebook with supporting code and markup analysis where appropriate. During this coursework you will:\n",
    "\n",
    "- Understand the specific business forecast task \n",
    "- Prepare a dataset, clean and impute where necessary \n",
    "- Train an ensemble classifier \n",
    "- Evaluate the performance and comment of success and failure modes\n",
    "- Complete all necessary stages of the data science process \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be around 100 words per ACTION cell, but use the wordcount over the duration of the Notebook at your discretion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Please use the below green cell, when writing your comments in markup.**\n",
    "* **Please feel free to add extra code cells in the notebook if needed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Title </b> (Optional)\n",
    "\n",
    "<p>Content</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Business Case Understanding\n",
    "\n",
    "### INTRODUCTION\n",
    "\n",
    "Over the last few years, the music industry has been dominated by digital streaming services, which produce vast amounts of data on listeners and their preferences. \n",
    "\n",
    "This has required major players in the industry to adopt a data driven approach to content delivery in order to stay competitive. \n",
    "\n",
    "Warner Music Group is looking to leverage its rich database to better understand the factors that have the most significant impact on the success of a new artist. This will allow them to optimize the allocation of resources when signing and promoting new artists.\n",
    "\n",
    "Warner’s (large) database contains several sources of data, including the streaming platforms Spotify, Amazon Live and Apple Music. \n",
    "\n",
    "For this case study, we will be looking using the Spotify dataset to predict the success of artists. In particular, we want to understand the role of Spotify playlists on the performance of artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Music\n",
    "\n",
    "When artists release music digitally, details of how their music is streamed can be closely monitored. \n",
    "\n",
    "Some of these details include:\n",
    "\n",
    "- How listeners found their music (a recommendation, a playlist)\n",
    "- Where and when (a routine visit to the gym, a party, while working). \n",
    "- On what device (mobile / PC)\n",
    "- And so on…\n",
    "\n",
    "Spotify alone *process nearly 1 billion streams every day* (Dredge, 2015) and this streaming data is documented in detail every time a user accesses the platform. \n",
    "\n",
    "Analyzing this data potentially enables us to gain a much deeper insight into customers’ listening behavior and individual tastes. \n",
    "\n",
    "Spotify uses it to drive their recommender systems – these tailor and individualize content as well as helping the artists reach wider and more relevant audiences. \n",
    "\n",
    "Warner Music would like to use it to better understand the factors that influence the *future success of its artists*, *identify potentially successful acts* early on in their careers and use this analysis to make resource decisions about how they market and support their artists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Spotify Playlists and why are relevant today?\n",
    "\n",
    "A playlist is a group of tracks that you can save under a name, listen to, and update at your leisure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'figure/spotify_playlist_image.png' width=\"50%\"  align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1. Screen shot of Spotify product show artists and playlists.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spotify currently has more than two billion publicly available playlists, many of which are curated by Spotify’s in-house team of editors. \n",
    "\n",
    "The editors scour the web on a daily basis to remain up-to-date with the newest releases, and to create playlists geared towards different desires and needs. \n",
    "\n",
    "Additionally, there are playlists such as [Discover Weekly](https://www.spotify.com/uk/discoverweekly/) and [Release Radar](https://support.spotify.com/uk/using_spotify/playlists/release-radar/) that use self-learning algorithms to study a user’s listening behavior over time and recommend songs tailored to his/her tastes.\n",
    "\t\n",
    "The figure below illustrates the progression of artists on Spotify Playlists:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'figure/playlist_heirarchy.png' width=\"80%\"  align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2. Figure to illustarte selecting artists and building audience profiles over progressively larger audiences of different playlists. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artist pool starts off very dense at the bottom, as new artists are picked up on the smaller playlists, and thins on the way to the top, as only the most promising of them make it through to more selective playlists. The playlists on the very top contain the most successful, chart-topping artists.\n",
    "\n",
    "An important discovery that has been made is that certain playlists have more of an influence on the popularity, stream count and future success of an artist than others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'figure/playlist_lift.png' width=\"80%\"  align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Figure 3. Figure to illustrate taking song stream data and using it to predict the trajectory, and likely success, of Warner artists. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, some playlists have been seen to be pivotal in the careers of successful artists. Artists that do make it onto one of these *key* playlists frequently go on to become highly ranked in the music charts. \n",
    "\n",
    "It is the objective of Warner’s [A&R](https://en.wikipedia.org/wiki/Artists_and_repertoire) team to identify and sign artists before they achieve this level of success i.e. before they get selected for these playlists, in order to increase their ROI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUSINESS PROBLEM → DATA PROBLEM\n",
    "\n",
    "Now that we have a better understanding of the business problem, we can begin to think about how we could model this problem using data. \n",
    "\n",
    "The first thing we can do is defining a criterion for measuring artist success. \n",
    "\n",
    "Based on our business problem, one way in which we can do this is to create a binary variable representing the success / failure of an artist and determined by whether a song ends up on a key playlist (1), or not (0). We can then generate features for that artist to determine the impact they have on the success of an artist.\n",
    "\n",
    "Our problem thus becomes a classification task, which can be modeled as follows:\n",
    "\n",
    "### *Artist Feature 1 + Artist Feature 2 …. + Artist Feature N = Probability of Success*\n",
    "\n",
    "where,\n",
    "\n",
    "**Success (1) = Artist Features on Key Playlist**\n",
    "\n",
    "The key playlists we will use for this case study are the 4 listed below, as recommended by Warner Analysts:\n",
    "\n",
    "1.\tHot Hits UK\n",
    "2.\tMassive Dance Hits\n",
    "3.\tThe Indie List\n",
    "4.\tNew Music Friday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coursework task is to take a look at the Spotify dataset to see how we might be able to set up this classification model.\n",
    "\n",
    "Complete the code sections below to work through the project from start to finish. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Project Template\n",
    "\n",
    "# WEEK 2\n",
    "\n",
    "# 1. Prepare Problem\n",
    "# a) Load libraries\n",
    "# b) Load dataset\n",
    "\n",
    "# 2. Summarize Data\n",
    "# a) Descriptive statistics\n",
    "# b) Data visualizations\n",
    "\n",
    "# WEEK 3\n",
    "\n",
    "# 3. Prepare Data\n",
    "# a) Data Cleaning\n",
    "# b) Feature Selection\n",
    "# c) Data Transforms (PCA, missing values, multi-collinearity, normalizing, class balance)\n",
    "\n",
    "# WEEK 5\n",
    "\n",
    "# 4. Evaluate Algorithms\n",
    "# a) Split-out validation dataset\n",
    "# b) Test options and evaluation metric\n",
    "# c) Spot Check Algorithms\n",
    "# d) Compare Algorithms\n",
    "\n",
    "# 5. Finalize Model\n",
    "# a) Predictions on validation dataset\n",
    "# b) Create standalone model on entire training dataset\n",
    "# c) Save model for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Guidance </b> \n",
    "\n",
    "<p>If you need to do something, instructions will appear in a box like this</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <span style=\"color:orange\"> WEEK 2</span>\n",
    "# <span style=\"color:orange\"> Submission Deadline: 30.01.2020</span>\n",
    "## 1. Prepare the problem \n",
    "\n",
    "Run your code on Faculty. We have prepared some of the data for you already. \n",
    "\n",
    "In addition, we have imported a custom module (spotfunc.py) containing useful functions written for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preamble \n",
    "\n",
    "# import sherlockml.filesystem as sfs\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#sfs.get('/input/spotfunc.py', 'spotfunc.py')\n",
    "#sfs.get('/input/playlists_ids_and_titles.csv', 'playlists_ids_and_titles.csv')\n",
    "#sfs.get('/input/new artists2015onwards.csv', 'newartists2015onwards.csv')\n",
    "#sfs.get('/input/cleaned_data.csv', 'cleaned_data.csv')\n",
    "\n",
    "# Add more stuff here as necessary \n",
    "\n",
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Import custom functions from library, named 'spotfunc'\n",
    "#import spotfunc as spotfunc_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import collections\n",
    "import networkx as nx\n",
    "import seaborn\n",
    "import warnings\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import data \n",
    "import csv\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Data Understanding\n",
    "\n",
    "<br>\n",
    "A year’s worth of Spotify streaming data in the WMG database amounts to approximately 50 billion rows of data i.e. 50 billion streams (1.5 to 2 terabytes worth), with a total of seven years of data stored altogether (2010 till today).\n",
    "\n",
    "For the purposes of this case study, we will be using a sample of this data. The dataset uploaded on the Faculty server is about 16GB, containing data from 2015 - 2017. Given the limits on RAM and cores, we will be taking a further sample of this data for purposes of this case study: a 10% random sample of the total dataset, saved as 'cleaned_data.csv'. \n",
    "\n",
    "*Note: The code for this sampling in included below, but commented out.*\n",
    "\n",
    "We can begin with reading in the datasets we will need. We will be using 2 files: \n",
    "1. Primary Spotify dataset\n",
    "2. Playlist Name Mapper (only playlist IDs provided in primary dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# Sampling data to read in 10% \n",
    "# sfs.get('/input/all_artists_with_date_time_detail.csv', 'client-data.csv')\n",
    "# # Read in data\n",
    "# # The data to load\n",
    "# f = 'client-data.csv'\n",
    "\n",
    "# # Count the lines\n",
    "# num_lines = sum(1 for l in open(f))\n",
    "# n = 10\n",
    "# # Count the lines or use an upper bound\n",
    "# num_lines = sum(1 for l in open(f))\n",
    "\n",
    "# # The row indices to skip - make sure 0 is not included to keep the header!\n",
    "# skip_idx = [x for x in range(1, num_lines) if x % n != 0]\n",
    "# # Read the data\n",
    "# data = pd.read_csv(f, skiprows=skip_idx )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (2,13) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 3805499\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read in sampled data\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "print('rows:',len(data))\n",
    "\n",
    "# Keep a copy of original data in case of changes made to dataframe\n",
    "all_artists = data.copy()\n",
    "\n",
    "# Load laylist data\n",
    "#playlist_ids_and_titles = pd.read_csv('playlists_ids_and_titles.csv',encoding = 'latin-1',error_bad_lines=False,warn_bad_lines=False)\n",
    "\n",
    "# Keep only those with 22 characters (data cleaning)\n",
    "#playlist_mapper = playlist_ids_and_titles[playlist_ids_and_titles.id.str.len()==22].drop_duplicates(['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by taking a look at what the Spotify data looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Inspect the data </b> \n",
    "\n",
    "Make sure you understand the data. Use methods like **data.head()**, **data.info()**, etc.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the data is a unique stream – every time a user streams a song in the Warner Music catalogue for at least 30 seconds it becomes a row in the database. Each stream counts as a ‘transaction’, the value of which is £0.0012, and accordingly, 1000 streams of a song count as a ‘sale’ (worth £1) for the artist. The dataset is comprised of listeners in Great Britain only.\n",
    "\n",
    "Not all the columns provided are relevant to us. Lets take a look at some basic properties of the dataset, and identify the columns that are important for this study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns you should *focus* on for this case study are:\n",
    "\n",
    "* Log Time – timestamp of each stream\n",
    "* Artist Name(s) – some songs feature more than one artist\n",
    "* Track Name\n",
    "* ISRC - (Unique code identifier for that version of the song, i.e. radio edit, album version, remix etc.)\n",
    "* Customer ID\n",
    "* Birth Year\n",
    "* Location of Customer\n",
    "* Gender of Customer\n",
    "* Stream Source URI – where on Spotify was the song played – unique playlist ID, an artist’s page, an album etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>day</th>\n",
       "      <th>log_time</th>\n",
       "      <th>mobile</th>\n",
       "      <th>track_id</th>\n",
       "      <th>isrc</th>\n",
       "      <th>upc</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>...</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekday_name</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>playlist_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>('small_artists_2016.csv', 9)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T12:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>('small_artists_2016.csv', 19)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T12:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>('small_artists_2016.csv', 29)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T14:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>('small_artists_2016.csv', 39)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T10:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>('small_artists_2016.csv', 49)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T10:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                  Unnamed: 0.1.1  day  \\\n",
       "0           0             9   ('small_artists_2016.csv', 9)   10   \n",
       "1           1            19  ('small_artists_2016.csv', 19)   10   \n",
       "2           2            29  ('small_artists_2016.csv', 29)   10   \n",
       "3           3            39  ('small_artists_2016.csv', 39)   10   \n",
       "4           4            49  ('small_artists_2016.csv', 49)   10   \n",
       "\n",
       "            log_time  mobile                          track_id          isrc  \\\n",
       "0  20160510T12:15:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "1  20160510T12:15:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "2  20160510T14:00:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "3  20160510T10:45:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "4  20160510T10:15:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "\n",
       "            upc       artist_name  ... hour minute week month  year  \\\n",
       "0  7.567991e+10  Sturgill Simpson  ...   12     15   19     5  2016   \n",
       "1  7.567991e+10  Sturgill Simpson  ...   12     15   19     5  2016   \n",
       "2  7.567991e+10  Sturgill Simpson  ...   14      0   19     5  2016   \n",
       "3  7.567991e+10  Sturgill Simpson  ...   10     45   19     5  2016   \n",
       "4  7.567991e+10  Sturgill Simpson  ...   10     15   19     5  2016   \n",
       "\n",
       "         date weekday  weekday_name playlist_id playlist_name  \n",
       "0  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "1  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "2  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "3  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "4  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3805499 entries, 0 to 3805498\n",
      "Data columns (total 45 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Unnamed: 0         int64  \n",
      " 1   Unnamed: 0.1       int64  \n",
      " 2   Unnamed: 0.1.1     object \n",
      " 3   day                int64  \n",
      " 4   log_time           object \n",
      " 5   mobile             bool   \n",
      " 6   track_id           object \n",
      " 7   isrc               object \n",
      " 8   upc                float64\n",
      " 9   artist_name        object \n",
      " 10  track_name         object \n",
      " 11  album_name         object \n",
      " 12  customer_id        object \n",
      " 13  postal_code        object \n",
      " 14  access             object \n",
      " 15  country_code       object \n",
      " 16  gender             object \n",
      " 17  birth_year         float64\n",
      " 18  filename           object \n",
      " 19  region_code        object \n",
      " 20  referral_code      float64\n",
      " 21  partner_name       object \n",
      " 22  financial_product  object \n",
      " 23  user_product_type  object \n",
      " 24  offline_timestamp  float64\n",
      " 25  stream_length      float64\n",
      " 26  stream_cached      float64\n",
      " 27  stream_source      object \n",
      " 28  stream_source_uri  object \n",
      " 29  stream_device      object \n",
      " 30  stream_os          object \n",
      " 31  track_uri          object \n",
      " 32  track_artists      object \n",
      " 33  source             float64\n",
      " 34  DateTime           object \n",
      " 35  hour               int64  \n",
      " 36  minute             int64  \n",
      " 37  week               int64  \n",
      " 38  month              int64  \n",
      " 39  year               int64  \n",
      " 40  date               object \n",
      " 41  weekday            int64  \n",
      " 42  weekday_name       object \n",
      " 43  playlist_id        object \n",
      " 44  playlist_name      object \n",
      "dtypes: bool(1), float64(7), int64(9), object(28)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                 0\n",
       "Unnamed: 0.1               0\n",
       "Unnamed: 0.1.1             0\n",
       "day                        0\n",
       "log_time                   0\n",
       "mobile                     0\n",
       "track_id                   0\n",
       "isrc                       4\n",
       "upc                        0\n",
       "artist_name                0\n",
       "track_name                 0\n",
       "album_name                 0\n",
       "customer_id                0\n",
       "postal_code          1352181\n",
       "access                     0\n",
       "country_code               0\n",
       "gender                 40422\n",
       "birth_year             10021\n",
       "filename                   0\n",
       "region_code           261956\n",
       "referral_code        3805499\n",
       "partner_name         3378646\n",
       "financial_product    2329099\n",
       "user_product_type      22992\n",
       "offline_timestamp    3805499\n",
       "stream_length              0\n",
       "stream_cached        3805499\n",
       "stream_source              0\n",
       "stream_source_uri    2761628\n",
       "stream_device              0\n",
       "stream_os                  0\n",
       "track_uri                  0\n",
       "track_artists              0\n",
       "source               3805499\n",
       "DateTime                   0\n",
       "hour                       0\n",
       "minute                     0\n",
       "week                       0\n",
       "month                      0\n",
       "year                       0\n",
       "date                       0\n",
       "weekday                    0\n",
       "weekday_name               0\n",
       "playlist_id          2761628\n",
       "playlist_name        2826389\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.artist_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The number of free user is {}'.format(sum(data.access==\"free\")))\n",
    "print('The number of premium user is {}'.format(sum(data.access==\"premium\")))\n",
    "print('The number of free user is {}'.format(sum(data.access==\"basic-desktop\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The number of user using Android is {}'.format(sum(data.stream_os==\"Android\")))\n",
    "print('The number of user using iOS is {}'.format(sum(data.stream_os==\"iOS\")))\n",
    "print('The number of user using Windows is {}'.format(sum(data.stream_os==\"Windows\")))\n",
    "print('The number of user using Mac is {}'.format(sum(data.stream_os==\"Mac\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Inspect the data </b> \n",
    "\n",
    "<p>Learn more about the basic information of the statistical data and the column names, so as to find out the useful information. Find missing values and interested values for our next step.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLORATORY ANALYSIS AND PLOTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the data set in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Exploratory analysis </b> \n",
    "\n",
    "<p>As demonstrated in class, explore various distribution of the data. Comment on any patterns you can see.</p>\n",
    "\n",
    "<p>- Highlight on any potential uncertainties or peculiarities that you observe. </p> \n",
    "\n",
    "<p>- Variables you might explore, include, but are not limited to: Age, Gender, Stream counts and playlists.</p>\n",
    "\n",
    "<p> - Use figures, plots and visualization as necessary.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#my interest column name\n",
    "my_list=[\"stream_length\",\"log_time\",\"artist_name\",\"year\",\"mobile\",\"access\",\"playlist_id\",\"playlist_name\",\"track_name\",\"isrc\",\"customer_id\",\"birth_year\",\"region_code\",\"gender\",\"stream_source_uri\"]\n",
    "data1=data[my_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at the first five rows\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1['gender'].value_counts() #look at how many female and male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "female_male=[1955719,1809358] \n",
    "user=['female','male'] \n",
    "colors=['r','y'] \n",
    "plt.pie(female_male, labels=user, colors=colors,startangle=90,autopct='%.1f%%') \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> The Gender ratio of users </b> \n",
    "\n",
    "<p>We want to see the gender ratio of users. From the pie chart above, we can see that 51.9% of users are female and 48.1% are male.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Age of User Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1['birth_year'].hist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#add a new column\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "data1['user_age']=2020-data1['birth_year']\n",
    "data1['user_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_cloumn=[\"stream_length\",\"log_time\",\"artist_name\",\"year\",\"mobile\",\"access\",\"playlist_id\",\"playlist_name\",\"track_name\",\"isrc\",\"customer_id\",\"birth_year\",\"region_code\",\"gender\",\"stream_source_uri\",\"user_age\"]\n",
    "data_new=data1[new_cloumn]\n",
    "data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_new['user_age'].hist()\n",
    "plt.figure(figsize=(9,5))\n",
    "X1=data_new.groupby(\"user_age\")['customer_id'].count()\n",
    "X1.plot()\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Users age </b> \n",
    "\n",
    "<p>From the bar chart, we can see that most users were born between 1990 and 2000, which means that most users are \n",
    "between 20 and 30 years old.Spotify is very popular with young people.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relatinship between the gender and age of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Birth year by gender \n",
    "seaborn.catplot(x=\"user_age\", hue=\"gender\",             \n",
    "            data=data_new[data_new['customer_id'].isin(data_new['customer_id'].unique())], kind=\"count\",             \n",
    "            height=8.20, aspect=14/8.27, palette=\"muted\") \n",
    "plt.xticks(rotation=270, fontsize=8) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> gender and age </b> \n",
    "\n",
    "<p>This graph shows that there are more female users than male users in the lower age bands, but the situation reverses in higher age bands.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mobile VS non-mobile user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " data1['mobile'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moble_not=[2943869,861630] \n",
    "user=['True','False'] \n",
    "colors=['g','y'] \n",
    "plt.pie(moble_not, labels=user, colors=colors,startangle=90,autopct='%.1f%%') \n",
    "plt.show "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Mobile </b> \n",
    "\n",
    "<p>Mobile phones are more popular because they are easy to carry.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paying users and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data1['access'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#deal with \"delected\"\n",
    "data_access = data[data.access != \"deleted\"]\n",
    "access = data_access[\"access\"].value_counts() \n",
    "access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot access pie chart \n",
    "# Add labels\n",
    "labels = [\"premium\", \"free\", \"basic-desktop\"] \n",
    "access_pie = data_access[\"access\"].value_counts().plot(kind=\"pie\", label=\"Access Ways\",                                                \n",
    "                                                autopct='%1.1f%%', shadow=True, startangle=90) \n",
    "access_pie.legend(labels, loc=\"best\") # Add legends \n",
    "\n",
    "access_pie.set_title(\"Premium vs Others\", fontsize=16) # Add title \n",
    "access_pie.axis(\"equal\")  # Equal aspect ratio ensures that pie is drawn as a circle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Premium vs Others </b> \n",
    "\n",
    "<p>From the pie above, 70.3% of customers are premium, Others are belong to free users. So there are huge potential to increase the revenue of spotify.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### premium service paying  through different methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the number of customers using different payment methods for premium service \n",
    "data_F = data[data.financial_product != \"deleted\"] \n",
    "# Drop the missing value \n",
    "financial_P = data_F[\"financial_product\"].value_counts() \n",
    "# Number of customers with premium service paying it through different methods \n",
    "financial_P "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot payment method bar chart \n",
    "financial_bar = data_F[\"financial_product\"].value_counts().plot(kind=\"bar\") \n",
    "financial_bar.set_title(\"Financial payment of users\", fontsize=16) \n",
    "# Add title "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Payment methods </b>\n",
    "\n",
    "<p>In terms of the plan users subscribe to, most paid users subscribe to ‘student’ plan. Firstly, such subscription profile corresponds to the age profile of Spotify users. Secondly, this is potentially an evidence that users are very sensitive to price. </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating functions for drawing histogram plots \n",
    "\n",
    "def categorical_plot(variable,xlabel,ylabel,title): # plotting categorical variables \n",
    "    seaborn.set(style=\"darkgrid\")     \n",
    "    ax = seaborn.countplot(x=variable ,data=data)     \n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=45)     \n",
    "         \n",
    "    plt.ylabel(ylabel)  #plt.xlabel(xlabel)   \n",
    "    plt.title(title)     \n",
    "    plt.show(ax) \n",
    "    #Functon for plotting of interval variables \n",
    "def interval_plot(variable,xlabel,ylabel,title,bins):     \n",
    "    a=seaborn.distplot(data[variable], hist=True, kde=False,\n",
    "                 bins=bins, color = 'red',                 \n",
    "                 hist_kws={'edgecolor':'black'})   \n",
    "    plt.xlabel(xlabel)     \n",
    "    plt.ylabel(ylabel)    \n",
    "    plt.title(title)     \n",
    "    plt.show(a) \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " categorical_plot('weekday_name','Weekday','Count','Distribution of Weekday') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Distribution of Weekend </b> \n",
    "\n",
    "<p>One interesting fact we observe from the dataset was that people listen to music more on certain days of a week. People listen to music more on Monday, Wednesday, Friday and Saturday significantly more than on the rest of the week.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of songs per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = [2014,2015,2016,2017]#Abscissa\n",
    "k1 = [1102,205293,1727360,1871744]#Y-axis\n",
    "plt.plot(x,k1,'s-',color = 'r',label=\" the number of songs per year\")#s-:square\n",
    "plt.xlabel(\"year\")# X name\n",
    "plt.ylabel(\"count\")#Y name\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> the number of songs per year </b> \n",
    "\n",
    "<p>Spotify’s user community saw a constant growth since 2014. In particular, the fastest growth was in 2015, where the number of Spotify users grew nearly 600%. The slowdown after 2015 was probably attributable to the introduction of one of Spotify’s major competitors: Apple music. \n",
    " </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top5 Loving music regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define top.5 loving music regions\n",
    "region_lovemusice=data1['region_code'].value_counts() \n",
    "region_lovemusice[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot top.5 loving music regions \n",
    "region_lovemusice_bar = region_lovemusice[:5].plot(kind=\"bar\") \n",
    "region_lovemusice_bar.set_title(\"Top 5 Most loving musice regions \", fontsize=14) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Loving music regions </b> \n",
    "\n",
    "<p>London plays the most music among UK cities. How much music is played is highly correlated with the population of the city. London is the largest city in the UK, with about 4 times the population of the second largest city: Manchester. Therefore, the large volume of music played in London is, to a large degree, explained by the population residing in this city. </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Most Streamed Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define top.10 popular artists\n",
    "artists = data[\"artist_name\"].value_counts() \n",
    "artists[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot top.10 popular artists popular artists \n",
    "artists_bar = artists[:10].plot(kind=\"bar\") \n",
    "artists_bar.set_title(\"Top 10 Most Popular Artists\", fontsize=14) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Popular artists </b> \n",
    "\n",
    "<p>Charlie Puth tops the ‘Most popular artist’ list, followed by Dua Lipa and Lukas Graham. \n",
    " </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seaborn.boxplot(data_new['user_age'],orient='v',color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.boxplot(data_new['stream_length'],orient='v',color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Box-plot for user age and stream length </b> \n",
    "\n",
    "<p>Most of Spotify’s users fall into the age band of 22-35. The medium of user age is 28, lower than 30. This means the majority of Spotify’s users are relatively young, with more born after 1990 than not. Spotify is yet to reach the elder population. The data does not seem entirely accurate, with some outliers at the age of 140, or 0. This brings the quality of the dataset to our attention: some users do not correctly state their ages. \n",
    "The median of stream time is 200 minutes. 50% users stream 180 to 220 minutes on one go. Most streams last from 100 to 300 minutes. Some long streams can last up to 950 minutes. \n",
    " </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <span style=\"color:orange\"> WEEK 3 Assignment</span>\n",
    "# <span style=\"color:orange\"> Submission Deadline: 13.02.2020</span>\n",
    "\n",
    "## 3.\tData Preperation and Feature Engineering\n",
    "<br>\n",
    "From our business understanding, we know that our criteria for success is whether or not an artist has been on one of 4 key playlists.  The column ‘stream_source_uri’, contains data about the source of the stream – whether it was from an artist’s page, an album, a playlist etc. \n",
    "\n",
    "For streams coming from different playlists, only the Spotify URI code is provided. To make sense of this column and identify our key playlists, we can use the additional table provided that we cleaned above and named 'playlist_mapper'.\n",
    "\n",
    "We can being by out data preperation by subsetting the 4 key playlists we are interested in and creating our dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# Read in sampled data\n",
    "data = pd.read_csv('cleaned_data.csv')\n",
    "print('rows:',len(data))\n",
    "\n",
    "# Keep a copy of original data in case of changes made to dataframe\n",
    "all_artists = data.copy()\n",
    "\n",
    "# select the useful columns\n",
    "focuses = ['log_time', 'artist_name', 'track_name', 'isrc', 'customer_id', \n",
    "         'birth_year', 'postal_code', 'country_code', \n",
    "         'gender', 'stream_source_uri', \n",
    "           'hour', 'playlist_id', 'playlist_name', \n",
    "           'stream_length', 'year', 'region_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load laylist data\n",
    "playlist_ids_and_titles = pd.read_csv('playlists_ids_and_titles.csv',encoding = 'latin-1',error_bad_lines=False,warn_bad_lines=False)\n",
    "'''\n",
    "error_bad_lines : bool, default True\n",
    "    Lines with too many fields (e.g. a csv line with too many commas) will by\n",
    "    default cause an exception to be raised, and no DataFrame will be returned.\n",
    "    If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
    "    returned.\n",
    "warn_bad_lines : bool, default True\n",
    "    If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
    "    \"bad line\" will be output.\n",
    "'''\n",
    "# Keep only those with 22 characters (data cleaning)\n",
    "playlist_mapper = playlist_ids_and_titles[playlist_ids_and_titles.id.str.len()==22].drop_duplicates(['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_ids_and_titles.id.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "playlist_ids_and_titles.name.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_artists.playlist_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "playlist_ids_and_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(playlist_ids_and_titles.name.value_counts())\n",
    "print('\\n')\n",
    "print(playlist_ids_and_titles.id.value_counts())\n",
    "print('\\n')\n",
    "print(playlist_mapper.id.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>There exist a large quantities of entries in playlist_ID where the value ‘name’ are labelled as ‘?’s, but their ID are unique. Therefore, it would be more accurate to use ID instead of names.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Dependent Variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Dependant variable </b> \n",
    "\n",
    "<p> Set up the problem as one of classification, selecting the relevant playlists as the variable we are trying to model.</p>\n",
    "\n",
    "<p> Write useful helper functions to support creating of the feature vector and target vector </p>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 key Playlists\n",
    "\n",
    "# select relevant playlists \n",
    "\n",
    "# Define Dependent Variable\n",
    "\n",
    "# def get_successful_artists(data):\n",
    "  \n",
    "# def get_successful_before_2017(data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas version 0.25.0\n",
    "\n",
    "def solve_multi_name(df, keyword=\",\", column=\"verbatimEventDate\"):\n",
    "    \"\"\"split on keyword in column for an enumeration and create extra record\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame with a double field entry in one or more values\n",
    "    keyword: str\n",
    "        word/character to split the double records on\n",
    "    column: str\n",
    "        column name to use for the decoupling of the records\n",
    "    \"\"\"\n",
    "    df[column] = df[column].str.split(keyword)\n",
    "    df = df.explode(column)\n",
    "    \n",
    "    df[column] = df[column].str.strip()  # remove white space around the words\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in row (the artists)  \n",
    "all_artists_split = solve_multi_name(all_artists.copy(), keyword='&', column='artist_name')\n",
    "\n",
    "print(all_artists.artist_name.str.contains('&').sum())\n",
    "print(all_artists.artist_name.shape)\n",
    "print(all_artists_split.artist_name.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>We observed that in some of the entries we have more than one artist. The artists names are connected with the sign ‘&’. Therefore, we will break any entries with ‘&’ in the variable ‘artist name’ into two entries.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successful_artists(data, Playlists_id): \n",
    "    # change the index to artist\n",
    "    # data.set_index('artist_name', append=False, inplace=True, drop=False)\n",
    "    \n",
    "    # select relevant playlists \n",
    "    relevant_playlists = data['playlist_id'].isin(Playlists_id)\n",
    "    \n",
    "    # Playlists = '|'.join(Playlists)\n",
    "    # relevant_playlists = data['playlist_name'].str.contains(Playlists)\n",
    "    # relevant_playlists = data['playlist_name'].isin(Playlists) # ignored the multi playlists!\n",
    "\n",
    "    # Define Dependent Variable\n",
    "    data['success'] = relevant_playlists\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_successful_before_2017(data):\n",
    "    \n",
    "    data = data.loc[data['year'] <= 2017] \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4 key Playlists\n",
    "key_playlists=[\"Hot Hits UK\",\"Massive Dance Hits\",\"The Indie List\",\"New Music Friday\"]\n",
    "\n",
    "# select relevant playlists id\n",
    "Playlists_id_mask = playlist_mapper[\"name\"].str.lower().isin(x.lower() for x in key_playlists) # case insensitive \n",
    "Playlists_id = playlist_mapper[Playlists_id_mask]\n",
    "Playlists_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the list of the id \n",
    "df_new = get_successful_artists(all_artists_split.copy(), Playlists_id.id.tolist())\n",
    "df_new\n",
    "success_artist = df_new.loc[df_new['success']==True]['artist_name']\n",
    "success_artist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new = get_successful_before_2017(df_new)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_new.shape)\n",
    "print(df_new.columns)\n",
    "print(df_new.success.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new.artist_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Dependent Variable </b> \n",
    "\n",
    "<p>成功多少We can create the dependent variable \"success\". If success has a value of 0, the artist cannot be predicted to succeed because it is not in the key playlist. A value of 1 for success means that the artist can be predicted to be successful.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our dependent variable – whether an artist is successful or not, we can look at generating a set of features, based on the columns within our dataset, that we think might best explain the reasons for this success. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE ENGINEERING**\n",
    "\n",
    "There are a large number of factors that could have an impact on the success of an artist, such as the influence of a playlist, or the popularity of an artist in a certain geographical region. \n",
    "\n",
    "To build a predictive model for this problem, we first need to turn these (largely qualitative) factors into measurable quantities. Characteristics like ‘influence’ and ‘popularity’ need to be quantified and standardized for all artists, to allow for a fair comparison. \n",
    "\n",
    "The accurateness of these numerical estimates will be the fundamental driver of success for any model we build. \n",
    "There are many approaches one might take to generate features. Based on the data columns available to us, a sensible approach is to divide our feature set into three groups:\n",
    "\n",
    "1.\tArtist Features\n",
    "2.\tPlaylist Features\n",
    "3.\tUser-base features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artist features\n",
    "\n",
    "*\tStream count\n",
    "*\tTotal Number of users\n",
    "*\tPassion Score \n",
    "\n",
    "The metric passion score is a metric suggested to us by Warner business analysts. \n",
    "\n",
    "It is defined as the number of stream divided by the total number of users. \n",
    "\n",
    "Warner analysts believe that repeated listens by a user is a far more indicative future success that simply total number of listens or total unique users. By including this in your model, we can evaluate whether this metric in fact might be of any significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Artist features </b> \n",
    "\n",
    "<p> Write useful functions to create these new features. </p>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream count per artist\n",
    "stream_count = df_new.groupby(['artist_name'])['artist_name'].count()\n",
    "# df_artist = pd.DataFrame(data=stream_count, columns=[\"artist_name\", \"Stream count\"])\n",
    "\n",
    "# convert the series to dataframe indexed by artist name\n",
    "stream_count = stream_count.to_frame()\n",
    "df_artist = stream_count.rename(columns={'artist_name':'stream_count'})\n",
    "df_artist.reset_index(inplace=True)\n",
    "\n",
    "# df_artist = stream_count.rename(columns={'index':'artist_name', 'artist_name':'stream_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users per artist\n",
    "# all_artists.groupby(['artist_name'])['customer_id'].count().reset_index(name='count').sort_values(['count'], ascending=False) \\\n",
    "Number_users = df_new.groupby(['artist_name'])['customer_id'].nunique().to_list() \n",
    "# Return number of unique elements in the object.\n",
    "\n",
    "df_artist['num_users'] = Number_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_new.customer_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passion Score\n",
    "df_artist['passion_score'] = df_artist['stream_count'] / df_artist['num_users']\n",
    "\n",
    "# merge playlist\n",
    "# df_artist_list = pd.merge(df_artist, all_artists[['artist_name', 'playlist_name']], on='artist_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream count per artist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users per artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passion Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playlist Features\n",
    "\n",
    "Understanding an artist’s growth as a function of his/her movement across different playlists is potentially key to understanding how to identify and breakout new artists on Spotify. \n",
    "\n",
    "In turn, this could help us identify the most influential playlists and the reasons for their influence.\n",
    "\n",
    "One way to model the effect of playlists on an artist’s performance has been to include them as categorical features in our model, to note if there are any particular playlists or combinations of playlists that are responsible for propelling an artist to future success:\n",
    "\n",
    "### *Artist Feature 1 + Artist Feature 2 …. + Artist Feature N = Probability of Success*\n",
    "**\n",
    "Success (1) = Artist Features on Key Playlist\n",
    "Failure (0) = Artist Not Featured on Key Playlist\n",
    "**\n",
    "\n",
    "Where,\n",
    "\n",
    "**\n",
    "⇒Artist Feature N = Prior Playlist 1 + Prior Playlist 2 +…Prior Playlist N\n",
    "**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have over 19,000 playlists in our dataset or 600 artists, using the playlists each artist has featured on, as categorical variables would lead to too many features and a very large, sparse matrix. \n",
    "\n",
    "Instead, we need to think of ways to summarize the impact of these playlists. One way to do this would be to consider the top 20 playlists each artist has featured on. \n",
    "\n",
    "Even better would be to come up with one metric that captures the net effect of all top 20 prior playlists, for each artist, rather including using all 20 playlists for each artists as binary variables. The intuition here is that if this metric as a whole has an influence on the performance of an artist, it would suggest that rather than the individual playlists themselves, it is a combination of their generalized features that affects the future performance of an artist. \n",
    "\n",
    "Accordingly, different combinations of playlists could equate to having the same impact on an artist, thereby allowing us to identify undervalued playlists.\n",
    "\n",
    "Some of the features such a metric could use is the number of unique users or ‘reach’, number of stream counts, and the passion score of each playlist\n",
    "\n",
    "* Prior Playlist Stream Counts\n",
    "* Prior Playlist Unique Users (Reach)\n",
    "* Prior Playlist Passion Score\n",
    "\n",
    "There are several other such features that you could generate to better capture the general characteristics of playlists, such as the average lift in stream counts and users they generate for artists that have featured on them. \n",
    "\n",
    "The code to calculate these metrics is provided below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Playlist features </b> \n",
    "\n",
    "<p> Write useful functions to create new playlist features, like those listed in the cell above. </p>\n",
    "\n",
    "<p> Are there other sensible ones you could suggest, work in your group to think about what other features might be useful and whether you can calculate them with the data you have </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using .groupby to 2 items simultantanously\n",
    "pasc = df_new.groupby(['artist_name', 'playlist_name'])['playlist_name'].count()\n",
    "panou = df_new.groupby(['artist_name', 'playlist_name'])['customer_id'].nunique()\n",
    "print(pasc)\n",
    "print(panou.shape)\n",
    "\n",
    "df_playlist = pasc.to_frame().rename(columns={'playlist_name':'stream_counts'})\n",
    "df_playlist['number_of_users'] = panou.to_list()\n",
    "df_playlist['passion_score'] = df_playlist['stream_counts'] / df_playlist['number_of_users']\n",
    "\n",
    "df_playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could divide up the work in the group by getting different people to calculate different features \n",
    "\n",
    "def playlist_avg_stream_counts(data):\n",
    "    pasc = data.groupby('playlist_name')['playlist_name'].count()\n",
    "    return pasc\n",
    "\n",
    "def playlist_avg_number_of_users(data):\n",
    "    panou = df_new.groupby('playlist_name')['customer_id'].nunique()\n",
    "    return panou\n",
    "\n",
    "def playlist_avg_passion_score(data):\n",
    "    data['passion_score'] = data['stream_count'] / data['num_users']\n",
    "    \n",
    "    return data\n",
    "\n",
    "# make sure you think they are actually being calculated correctly\n",
    "# how could you demonstrate the code you write is working correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pasc = playlist_avg_stream_counts(df_new)\n",
    "panou = playlist_avg_number_of_users(df_new)\n",
    "print(pasc.shape)\n",
    "print(panou.shape)\n",
    "\n",
    "# convert the series to dataframe indexed by artist name\n",
    "df_playlist = pasc.to_frame().rename(columns={'playlist_name':'stream_count'})\n",
    "df_playlist.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "df_playlist['num_users'] = panou.to_list()\n",
    "# passion score\n",
    "df_playlist = playlist_avg_passion_score(df_playlist)\n",
    "df_playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could divide up the work in the group by getting different people to calculate different features \n",
    "\n",
    "#def playlist_avg_stream_counts(data):\n",
    "\n",
    "\n",
    "#def playlist_avg_number_of_users(data):\n",
    "\n",
    "\n",
    "#def playlist_avg_passion_score(data):\n",
    "\n",
    "\n",
    "# make sure you think they are actually being calculated correctly\n",
    "# how could you demonstrate the code you write is working correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-base features\n",
    "\n",
    "We can use the age and gender columns to create an audience profile per artist.\n",
    "*\tGender Percentage Breakdown\n",
    "*\tAge vector quantization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: User features </b> \n",
    "\n",
    "<p> Write useful functions to create new user features, like those listed in the cell above. </p>\n",
    "\n",
    "<p> Are there other sensible ones you could suggest? Work in your group to think about what other features might be useful and whether you can calculate them with the data you have. Justify your reasoning. </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender breakdown\n",
    "df_gender = df_new.copy()\n",
    "\n",
    "def gender_percentage(df_gen, gender='male'):\n",
    "    if len(df_gen)!=0:\n",
    "        # count the number of na\n",
    "        num_na = df_gen.gender.isna().sum()\n",
    "        # assume half of nan gender as 'male' \n",
    "        \n",
    "        df_gen = df_gen.drop_duplicates(subset=['customer_id']) \n",
    "        perc = (df_gen[df_gen.gender==gender].shape[0] + num_na/2)/len(df_gen)\n",
    "        \n",
    "        return perc\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "gender_per = df_gender.groupby('artist_name').apply(gender_percentage)\n",
    "gender_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_artist['male_per'] = gender_per.to_list()\n",
    "df_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Age breakdown\n",
    "\n",
    "def age_mean(df):\n",
    "    df = df.dropna(subset = ['birth_year'])\n",
    "#     df = df[(df.birth_year!='male') & (df.birth_year!='female')]\n",
    "    age = df.year - df.birth_year\n",
    "    \n",
    "    df['age'] = age\n",
    "    \n",
    "    artist_age_mean = df.groupby('artist_name')['age'].mean()\n",
    "    \n",
    "    return artist_age_mean\n",
    "\n",
    "artist_age_mean = age_mean(df_new)\n",
    "\n",
    "# add the average age of users to artist\n",
    "df_artist['ave_age'] = artist_age_mean.to_list()\n",
    "\n",
    "df_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the success label to df_artist\n",
    "df_artist['success'] = df_artist['artist_name'].isin(success_artist)\n",
    "\n",
    "df_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age breakdown\n",
    "\n",
    "#def age_percentages(df):\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principle Component Analysis**\n",
    "\n",
    "The data also contains a partial region code of the listener. We might want to consider including the regional breakdown of streams per artist as a feature of our model, to know if streams for certain regions are particularly influential on the future performance of an artist. \n",
    "\n",
    "However, we have over 400 unique regions and like playlists, including them all would lead to too many features and a large sparse matrix. One way in which to extract relevant ‘generalized’ features of each region would be to incorporate census and demographic data, from publicly available datasets. \n",
    "\n",
    "This is however beyond the scope of this courswork. Instead, a better way to summarize the impact of regional variation in streams is to use dimensionality reduction techniques. Here we will use Principle Component Analysis (PCA) to capture the regional variation in stream count.\n",
    "\n",
    "PCA captures the majority of variation in the original feature set and represents it as a set of new orthogonal variables. Each ‘component’ of PCA is a linear combination of every feature, i.e. playlist in the dataset. Use **`scikit-learn`**’s PCA module (Pedregosa, et al., 2011) for generating PCA components.\n",
    "\n",
    "For a comprehensive understanding of how sklearn's PCA module works, please refer to the sklearn documentation. We will using 10 components of PCA in our model.\n",
    "\n",
    "*Note: We could also apply a similar method to condense variation in stream across the 19,600 different playlists in our dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: PCA features </b> \n",
    "\n",
    "<p> Write useful functions to create new user feature based on regions data. </p>\n",
    "\n",
    "<p> Are there other sensible features you could suggest? Work in your group to think about what other features might be useful and whether you can calculate them with the data you have. Justify your reasoning. </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_new.groupby(['artist_name','region_code']).count().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use Principle Component Analysis (PCA) to capture the regional variation in stream count.\n",
    "artist_region = df_new.groupby(['artist_name','region_code']).agg({'region_code': ['count']})\n",
    "\n",
    "feature_region = artist_region.unstack().fillna(0)\n",
    "feature_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "'''\n",
    "to normalize/standardize (mean = 0 and standard deviation = 1) your features/variables/columns of X\n",
    "'''\n",
    "scaler = StandardScaler()\n",
    "artist_region_scaled = scaler.fit_transform(feature_region)\n",
    "\n",
    "artist_region_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>\n",
    "We pivoted the index levels at the region code level, returning a DataFrame artist_region.unstack with a new level of column labels whose inner-most level consists of the pivoted index labels. All the missing values are assigned value 0. We then normalised the artist_region_unstack dataframe using standard Scaler.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region Code PCA\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "\n",
    "artist_region_pca = pca.fit(artist_region_scaled)\n",
    "\n",
    "print(artist_region_pca.components_.shape)\n",
    "\n",
    "# pca_regions_output \n",
    "print(artist_region_pca.n_components_)\n",
    "print(artist_region_pca.n_features_)\n",
    "print(artist_region_pca.n_samples_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>Linear dimensions need to be reduced, so we use decomposition tools to reduce dimensions. In our model, we will use 10 components of PCA, and use singular value decomposition of data to project it into a low-dimensional space to visualize PCA data frame.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<b>WARNING: PCA features </b> \n",
    "    \n",
    "<p>If you struggle to complete this section successfully <b>please email me</b> and we will provide code to compute the new features. This will help with performance of the classifier in the next stage.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the PCA feature table to make sure the dataframe looks as expected. Comment on anything the looks important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pca.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = {'pca_ouput_df': pca.components_.tolist()}\n",
    "pca_regions_output = pd.DataFrame(data=d)\n",
    "\n",
    "pca_regions_output['pca_ouput_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca_regions_output['pca_ouput_df'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: PCA plot </b> \n",
    "\n",
    "<p> Use a figure to show which components of PCA explain the majority of variation in the data. Accordingly, use only those components in your further analysis.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(artist_region_pca.components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "artist_region.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "generate a name list for the principle components and create a Dataframe for the region_pca and use the name list created above. Get the artist name in\n",
    "the region_stream_unstack dataframe and Concatenate artist_pca and region_pca dataframe along the columns.\n",
    "'''\n",
    "pc_name=[]\n",
    "for i in range(1,11):\n",
    "    pc_name.append('region_pc'+str(i))\n",
    "\n",
    "region_pcaDF = pd.DataFrame(data=np.transpose(artist_region_pca.components_), columns=pc_name)\n",
    "artist_pca = pd.Series(feature_region.index.values, name='artist_name')\n",
    "\n",
    "region_pcaDF = pd.concat([artist_pca, region_pcaDF],axis=1)\n",
    "\n",
    "# At last, we can plot the cumunative pca explained variance ratio according to number of components.\n",
    "plt.bar(pc_name,pca.explained_variance_ratio_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('principle components')\n",
    "plt.ylabel('explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>Adding up the explianed variance ratio of the 10 components, we observe that the total expleined variance ratio will only be greater than 90% if we include all 10 components. Therefore, we retain all 10 components for further analysis </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "artist_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data transformation**\n",
    "\n",
    "The final step is to decide whether or not to normalize/transform any of the features. \n",
    "\n",
    "We should normalize data if we are more interested in the relative rather than absolute differences between variables. Given that all the numerical features in our dataset (centrality, lift, influence, gender breakdown, age breakdown) were meaningful, i.e. distances did make a difference;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Feature transformation </b> \n",
    "\n",
    "<p> Comment on whether transforming particular features (influence, gender breakdown, age breakdown) is useful. Calculate the transformation where necessary.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we can combine all of our features that we generated above, into a dataframe that can be processed by a machine learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df = pd.DataFrame(variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "region_pcaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# variables \n",
    "final_df = pd.merge(df_artist, region_pcaDF, on=\"artist_name\",how=\"outer\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df.drop('success', axis=1).hist(bins=30,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "Before we can run any models on our dataset, we must make sure it is prepared and cleaned to avoid errors in results. This stage is generally refered to as preprocessing.\n",
    "\n",
    "To begin with, we need to deal with missing data in the dataframe - the ML algorithm will not be able to process NaN or missing values. \n",
    "\n",
    "For this study, we will be imputing missing numerical values, and filling any one which we were not able to imput, with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Missing values </b> \n",
    "\n",
    "<p> Use the <b>Imputer</b> class to alter your final Dataframe that contains your feature vector.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_drop = final_df.dropna()\n",
    "final_df_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "#from sklearn.preprocessing import Imputer\n",
    "\n",
    "#imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "\n",
    "#fill remaining nan with 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to make sure that none of the variables going into the model are collinear, and if so, we need to remove those variables that are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Multi-collinearity </b> \n",
    "\n",
    "<p> Check and deal with multi-collinearity in your feature set.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_drop.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicollinearity\n",
    "# Check for highly correlated variables (>90%)\n",
    "\n",
    "features=['stream_count', 'num_users', 'passion_score', 'male_per',\n",
    "       'ave_age', 'region_pc1', 'region_pc2', 'region_pc3',\n",
    "       'region_pc4', 'region_pc5', 'region_pc6', 'region_pc7', 'region_pc8',\n",
    "       'region_pc9', 'region_pc10']\n",
    "\n",
    "c = final_df[features].corr().abs()\n",
    "print(c)\n",
    "s = c.unstack()\n",
    "print('\\n ************************************ s  ****************** \\n')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_sorted = sorted(s.items(),key = lambda x: x[1], reverse=True)\n",
    "corr_sorted = [corr_sorted[x] for x in range(len(corr_sorted)) if corr_sorted[x][1]!=1]\n",
    "corr_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove one of highly correlated varibles (test removing other as well)\n",
    "temp = final_df_drop.drop('num_users',axis=1)\n",
    "temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We checked multicollinearity and dropped all variables that are significantly correlated. 'stream_count' and 'num_users' is highly related (0.996), so we dropped 'num_users'.</p> \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to take a look out the class balance in our dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the natural bias in our data, i.e. there are more cases of failure than of success in the training and test sets; there is a strong bias toward predicting ‘failure’. Based on our complete (unbalanced classes) training sample, if the model only predicted ‘failure’, we would achieve an accuracy of 88.8%. \n",
    "\n",
    "To give us a more even class balance, without losing too much data, we will sample data from the bigger class to achive a class balance closer to 60-40. \n",
    "\n",
    "There is another way to determine the accuracy of our predictions using a confusion matrix and ROC curve, but more on that later. For now, we will go ahead with sampling the bigger class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Class balance </b> \n",
    "\n",
    "<p> Calculate and comment on class balance.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "success_count = final_df_drop.success.value_counts()\n",
    "print('failure :', success_count[0])\n",
    "print('success :', success_count[1])\n",
    "print('Proportion:', round(success_count[0] / success_count[1], 2), ': 1')\n",
    "\n",
    "success_count.plot(kind='bar', title='Count (success)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_fail, count_suc  = final_df_drop.success.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_suc = final_df_drop[final_df_drop['success'] == 1]\n",
    "df_fail = final_df_drop[final_df_drop['success'] == 0]\n",
    "print(df_suc.shape)\n",
    "print(df_fail.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random under-sampling\n",
    "\n",
    "df_fail_under = df_fail.sample(int(count_suc/40*60))  # achive a class balance closer to 60-40.\n",
    "\n",
    "df_test_under = pd.concat([df_fail_under, df_suc], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_under.success.value_counts())\n",
    "\n",
    "df_test_under.success.value_counts().plot(kind='bar', title='Count (success)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Class balance\n",
    "final_df_drop.groupby('success').groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "<p>Data is split into training and validation set and resampling was performed in class balance part. </p> \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <span style=\"color:orange\"> WEEK 5 Assignment</span>\n",
    "# <span style=\"color:orange\"> Submission Deadline: 20.02.2020</span>\n",
    "## 4.\tEvaluate algorithms \n",
    "\n",
    "**Model Selection**\n",
    "\n",
    "There are number of classification models available to us via the **`scikit-learn`** package, and we can rapidly experiment using each of them to find the optimal model.\n",
    "\n",
    "Below is an outline of the steps we will take to arrive at the best model:\n",
    "\n",
    "*\tSplit data into training and validation (hold-out) set\n",
    "*\tUse cross-validation to fit different models to training set\n",
    "*\tSelect model with the highest cross-validation score as model of choice\n",
    "*\tTune hyper parameters of chosen model.\n",
    "*\tTest the model on hold-out set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION: Spot-check algorithms </b> \n",
    "\n",
    "<p> Try a mixture of algorithm representations (e.g. instances and trees). </p>\n",
    "\n",
    "<p> Try a mixture of learning algorithms (e.g. different algorithms for learning the same type of representation).<p>\n",
    "\n",
    "<p> Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and nonparametric).</p>\n",
    "\n",
    "<p> Divide this work up among the different members of your team and then compare and comment on the performance of various approaches.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# YOU CAN EXPERIMENT WITH CLASSIFIERS NOT EXPLICITLY COVERED IN CLASS \n",
    "\n",
    "# classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clf in classifiers:\n",
    "    #clf.fit(X, y)\n",
    "    # score = clf.score(X_test, y_test)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Present Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "To get a better idea of the quality of our predictions, we can plot a confusion matrix and ROC curve. \n",
    "\n",
    "\n",
    "A confusion matrix is a technique for summarizing the performance of a classification algorithm that allows visualization of the performance of an algorithm. \n",
    "\n",
    "Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). \n",
    "\n",
    "The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION:  Confusion matrix  </b> \n",
    "\n",
    "<p> Comment on the performance of your final algorithm. Repeat analysis from earlier in the Notebook if necessary. </p>\n",
    "\n",
    "<p> Explain confusion matrix results, calculate accuracy and precision etc. </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve \n",
    "\n",
    "Receiver Operating Characteristic (ROC) curves show the ability of the model to classify subjects correctly across a range of decision thresholds, i.e. it plots the True Positive Rate vs. False Positive Rate at every probability threshold. \n",
    "\n",
    "The AUC summarizes the results of an ROC – it is the probability that a randomly chosen ‘success’ example has a higher probability of being a success than a randomly chosen ‘failure’ example. A random classification would yield an AUC of 0.5, and a perfectly accurate one would yield 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION:  ROC Curve </b> \n",
    "\n",
    "<p> Comment on the performance of your final algorithm. Repeat analysis from earlier in the Notebook if necessary. </p>\n",
    "\n",
    "<p> Explain any observations about the ROC results. </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ROC curve\n",
    "\n",
    "# Plot classifier ROC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a validated model, we can potentially analyze the features of the model, to understand which ones have had the most impact on predicting an artist’s success. \n",
    "\n",
    "To do this, we can plot the feature importance as determined by the classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>ACTION:  Feature importance</b> \n",
    "\n",
    "<p> Where possible, comment on the feature selection and performance of your final algorithm. Repeat analysis from earlier in the Notebook if necessary. </p>\n",
    "\n",
    "<p> Explain any observations about the sensitivity of your final analysis. </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "**Please provide summaries of the work completed and the outcomes of the analysis**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips completing the coursework \n",
    "\n",
    "- **Faculty** - You are free to run the code on your local machine, but if training timings and memory become an issue then use Faculty to complete the coursework. Technical support for using Faculty will be provided as necessary. \n",
    "\n",
    "\n",
    "- **JIRA** - Assess the different potential work packages and break the overall objectives into a set of tasks and queue them up in the backlog column of the Kanban board. Create new tasks as and when necessary during the course of your analysis. \n",
    "\n",
    "\n",
    "- **Fast First Pass** - Make a first-pass through the project steps as fast as possible. This will give you confidence that you have all the parts that you need and a baseline from which to improve.\n",
    "\n",
    "\n",
    "- **Attempt Every Step** -  It is easy to skip steps, especially if you are not confident or familiar with the tasks of that step. Try and do something at each step in the process, even if it does not contribute to improved accuracy. You can always build upon it later. Don’t skip steps, just reduce their contribution.\n",
    "\n",
    "\n",
    "- **Ratchet Accuracy** - The goal of the project is to achieve relatively good model performance (which ever metric you use to measure this) and give you confidence about the ML project structure and workflow. Every step contributes towards this goal. Treat changes that you make as experiments that increase accuracy as the golden path in the process and reorganize other steps around them. Performance is a ratchet that can only move in one direction (better, not worse).\n",
    "\n",
    "\n",
    "- **Adapt As Needed** - Do not limit your analysis to the instructions provided in Guidelines cells, feel free to expand your analysis beyond them. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
