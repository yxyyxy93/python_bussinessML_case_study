{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <br style =\"font-family:UCL-SoM-Outline;color:#EA7600\"> GROUP H COURSEWORK </br> \n",
    "    </p>\n",
    "</h1>\n",
    "\n",
    "<div class=\"image\">\n",
    "\n",
    "<img src=\"./figure/som_ft.png\" width=\"70%\"  align=\"right\">\n",
    "<h4>\n",
    "          <p style=\"font-size:18pt\">MSIN0097 Predictive Analytics</p>\n",
    "          <p style=\"font-size:18pt;font-family:UCL-SoM-Solid; color:#EA7600;\">Student Name: <u>Thomas Zenkner, Ioana Buzelan, Bianca Ursulescu, Jiarui Xin, Christoph Loos</u> </p> \n",
    "          <p style=\"font-size:18pt;font-family:UCL-SoM-Solid; color:#EA7600;\">ID: <u>xxx</u> </p> \n",
    "          <p style=\"font-size:18pt;font-family:UCL-SoM-Solid; color:#EA7600;\">Email: <u>xxxx</u> </p> \n",
    "\n",
    "</h4>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSEWORK: WARNER MUSIC\n",
    "\n",
    "\n",
    "\n",
    "### PREDICTING THE SUCCESS OF ARTISTS ON SPOTIFY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Set-up](#1.Set-up)\n",
    "    * [1.1. Unique Artist Names, etc, etc,](#1.1.-Unique-Artist-Names,-etc,-etc,)\n",
    "    * [1.2. Connection to **`Spotify API`**](#1.1.-Connection-to-Spotify-API)\n",
    "* [2. Feature Engineering](#2.-Feature-Engineering)\n",
    "    * [2.1 Artist Features](#2.1-Artist-Features)\n",
    "    * [2.2 Playlist Features](#2.2-Playlist-Features)\n",
    "    * [2.3 User Features](#2.3-User-Features)\n",
    "    * [2.4 Merge the features into the analytics-ready DataFrame](#2.4-Merge-the-features-into-the-analytics-ready-DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Set-up\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: missingno in d:\\anaconda\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from missingno) (1.3.1)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (from missingno) (3.1.3)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from missingno) (1.18.1)\n",
      "Requirement already satisfied: seaborn in d:\\anaconda\\lib\\site-packages (from missingno) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->missingno) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->missingno) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->missingno) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib->missingno) (0.10.0)\n",
      "Requirement already satisfied: pandas>=0.22.0 in d:\\anaconda\\lib\\site-packages (from seaborn->missingno) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->missingno) (1.14.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->missingno) (45.1.0.post20200119)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\anaconda\\lib\\site-packages (from pandas>=0.22.0->seaborn->missingno) (2019.3)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.11 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0.post20200119)\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "!pip install missingno\n",
    "import missingno as msno\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "!pip install matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # Import custom functions from library, named 'spotfunc'\n",
    "# import spotfunc as spotfunc_v2\n",
    "\n",
    "# Ignore useless warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "# Various other things needed for data preparation\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows data: 3805499\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read in sampled data\n",
    "data = pd.read_csv('cleaned_data.csv', low_memory=False)\n",
    "print('rows data:',len(data))\n",
    "\n",
    "# Keep a copy of original data in case of changes made to dataframe\n",
    "all_artists = data.copy()\n",
    "\n",
    "# Load playlist data\n",
    "playlist_ids_and_titles = pd.read_csv('playlists_ids_and_titles.csv',encoding = 'latin-1',error_bad_lines=False,warn_bad_lines=False)\n",
    "\n",
    "# Drop duplicates\n",
    "playlist_mapper = playlist_ids_and_titles.drop_duplicates(['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DATA CLEANING \n",
    "#make all artists with lower case\n",
    "data['artist_name'] = data['artist_name'].astype(str).str.lower()\n",
    "data['track_name'] = data['track_name'].astype(str).str.lower()\n",
    "\n",
    "# As age is more intuitive to interpret than birthyear, we convert \n",
    "data[\"user_age\"] = 2019-data[\"birth_year\"]\n",
    "data['user_age'] = data['user_age'].apply(lambda x: x if x <= 100 else 100) #Ioana: anything greater than 100 will take the value of 100\n",
    "# Maybe instead of making <100 to 100 maybe make them the median or mean or mode?\n",
    "\n",
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df contianing only the 4 popular playlists and all their associated IDs\n",
    "filtered_mapper = playlist_mapper[playlist_mapper[\"name\"].isin([\"Hot Hits UK\", \"Massive Dance Hits\", \"The Indie List\", \"New Music Friday\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we look at what data type Pandas attributed to each cloumn\n",
    "# we need two options to force Pandas to count non-null instances in each coloumn\n",
    "data.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <b> Initial Overview of Data Load </b>\n",
    "\n",
    "<p>The first thing we observe is that the majority (28/45) of columns are non-numeric (neither int nor float Dtypes), and got classified as an object instead. While an object could, in theory, hold different values, we know that these objects hold text values due to the fact that they came from a .csv file (which can only hold text or numbers).\n",
    "    \n",
    "With that out of the way, we see that not all columns contain 3805499 non-null values (e.g. postal_code only has 2453318 and stream_source_uri only 1043871). This information will be important later on when we prepare the dataset for various ML tasks - we will need to deal with these Null values somehow. We also see that 4 columns (e.g. referral_code or stream_cached) have 0 values in them, which means that we will drop these columns, as they serve no purpose nor benefit.</p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chris\n",
    "# Because vizualising missing data is not possible due to data size\n",
    "for col in data.columns:\n",
    "    pct_missing = np.mean(data[col].isnull())\n",
    "    if pct_missing != 0:\n",
    "        print('{} - {}%'.format(col, round(pct_missing*100))) # look into financial product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping redundant columns\n",
    "data.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'], axis=1, inplace = True) # not adding any value\n",
    "data.drop(['referral_code' , 'offline_timestamp', 'stream_cached', 'source'], axis=1, inplace = True) # null columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Counting categorical values </b>\n",
    "\n",
    "<p>From counting and analysing the categorical values, we get the information that the location of users follows the population distribution of the UK's biggest cities (which is to be expected). We have circa 150.000 more streams from female customers opposed to males and know that most users use either Vodafone or Boku as their mobile broadband provider. The user_product_type contains a majority of paid users and most streams stem from mobile devices, more precisely mobile devices running iOS. Most Columns do not grant any meaningful insight, but they may still be of great use for our ML model later.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "rs = np.random.RandomState(10)\n",
    "\n",
    "f, axes = plt.subplots(2, 2, figsize=(15, 7))\n",
    "sns.despine(left=True)\n",
    "\n",
    "sns.distplot(data['user_age'], bins=50, fit=norm, ax=axes[0, 0])\n",
    "sns.distplot(data[\"stream_length\"], bins=50,fit=norm, ax=axes[0, 1])\n",
    "sns.distplot(data[\"hour\"], bins=24,fit=norm, ax=axes[1, 0])\n",
    "sns.distplot(data[\"weekday\"], bins=7, fit=norm, ax=axes[1, 1])\n",
    "\n",
    "axes[0,0].set_xlim(10, 100)\n",
    "axes[0,1].set_xlim(0,500)\n",
    "\n",
    "axes[0,0].set_title(\"Age Distribution\")\n",
    "axes[0,1].set_title(\"Stream Lenght Distribution\")\n",
    "axes[1,0].set_title(\"Hour Distribution\")\n",
    "axes[1,1].set_title(\"Weekday Distribution\")\n",
    "\n",
    "axes[0,0].set_xlabel('')\n",
    "axes[0,1].set_xlabel('')\n",
    "axes[1,0].set_xlabel('')\n",
    "axes[1,1].set_xlabel('')\n",
    "\n",
    "plt.setp(axes, yticks=[])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div >\n",
    "<b> Age distribution </b>\n",
    "\n",
    "<p>We see that the age distribution is very much right skewed in comparison to the normal distribution displayed, and has its mode around 23, which means that a large part of user is younger than the average of 28,9 years old. We also see users' age spread out all the way back until 70+ (we capped the age to 100 for this graph, datapoints above 100 are most likely artifacts and/or faulty data. We will deal with these later).\n",
    "Such a distribution was expected, as young users have much more rapidly adopted to new means of listening to music.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b> Stream lenght distribution </b>\n",
    "\n",
    "<p>Most streams are between 150 and 250 seconds long, with a few beeing noticeable longer (up to 800, truncated in this graph for readability purposes) and some more beeing noticeable shorter. The length starts at 30, as spotify does not count streaming for less than 30 seconds as a stream. The mode of 200 is to be expected, as the average song is about 3-4 minutes long, which is 180-240 seconds. </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b>Usage time distribution </b>\n",
    " \n",
    "\n",
    "<p>We can observe with the above three graphs that most users listen between the hours of 15:00 and 20:00. Further, we see a pattern that indicates that Mondays (day=0) and Saturdays (day=5) have the most streams, and the months of June, July and August also have the most streams.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('seaborn-white')\n",
    "\n",
    "labels = 'Male', 'Female'\n",
    "sizes = [sum(data[\"gender\"]!=\"female\"), sum(data[\"gender\"]==\"female\")]\n",
    "colors = [\"#1aa64b\",\"#363837\"]\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.title('Gender Distribution')\n",
    "plt.pie(sizes,  explode=(0.04,0), labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, pctdistance=1.2,labeldistance=1.4)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C: Implement threshold for all grpahs? 5 % cutoff seems reasonable\n",
    "\n",
    "f, axes = plt.subplots(4, 2, figsize=(20, 35))\n",
    "\n",
    "prob = data.stream_os.value_counts(normalize=True)\n",
    "threshold = 0.05\n",
    "mask = prob > threshold\n",
    "tail_prob = prob.loc[~mask].sum()\n",
    "prob = prob.loc[mask]\n",
    "prob['other'] = tail_prob\n",
    "prob.plot(kind='bar', color=[\"#1aa64b\",\"#363837\"], ax=axes[0, 0])\n",
    "axes[0,0].set_title(\"Operating System Distribution\")\n",
    "axes[0,0].set_xticklabels(prob.index, rotation=0)\n",
    "\n",
    "data_plot_1 = data.stream_device.value_counts(normalize=True)\n",
    "data_plot_1.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[0,1])\n",
    "axes[0,1].set_title(\"Device Distribution\")\n",
    "axes[0,1].set_xticklabels(data_plot_1.index, rotation=0)\n",
    "\n",
    "data_plot_2 = data.mobile.value_counts(normalize=True)\n",
    "data_plot_2.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[1,0])\n",
    "axes[1,0].set_title(\"Mobile Distribution\")\n",
    "axes[1,0].set_xticklabels(data_plot_2.index, rotation=0)\n",
    "\n",
    "data_plot_3 = data.partner_name.value_counts(normalize=True)\n",
    "data_plot_3.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[1,1])\n",
    "axes[1,1].set_title(\"Provider Distribution\")\n",
    "axes[1,1].set_xticklabels(data_plot_3.index, rotation=0)\n",
    "\n",
    "data_plot_4 = data.user_product_type.value_counts()\n",
    "data_plot_4.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[2,0])\n",
    "axes[2,0].set_title(\"User Product Type distribution\")\n",
    "axes[2,0].set_xticklabels(data_plot_4.index, rotation=0)\n",
    "\n",
    "data_plot_5 = data.stream_source.value_counts(normalize=True)\n",
    "data_plot_5.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[2,1])\n",
    "axes[2,1].set_title(\"Stream Source Distribution\")\n",
    "axes[2,1].set_xticklabels(data_plot_5.index, rotation=0)\n",
    "\n",
    "data_plot_6 = data.financial_product.value_counts(normalize=True)\n",
    "data_plot_6.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[3,0])\n",
    "axes[3,0].set_title(\"Product Distribution\")\n",
    "axes[3,0].set_xticklabels(data_plot_6.index, rotation=45)\n",
    "\n",
    "data_plot_7 = data.access.value_counts()\n",
    "data_plot_7.plot(kind=\"bar\", color=[\"#1aa64b\",\"#363837\"], ax=axes[3,1])\n",
    "axes[3,1].set_title(\"User Subscription Type Distribution\")\n",
    "axes[3,1].set_xticklabels(data_plot_7.index, rotation=0)\n",
    "\n",
    "#Do we want this? \n",
    "# data.region_code.value_counts()[:5].plot(kind=\"bar\", color=[\"C0\",\"C1\"], ax=axes[4,0])\n",
    "# axes[4,0].set_title(\"User Location Distribution\")\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b>User Demographics</b>\n",
    "\n",
    "<p>Our userbase contains slightly more females then males. The majority use an iOS mobile device, have either Vodafone or Boku as their mobile broadband provider and belong to the user category \"paid\". Most streaming is done from collections. Users mostly fall under the financial product category \"student\" and have a \"premium\" type subscription. Most of them stream form London. Such subscription profile corresponds to the age profile of Spotify users. This is also potentially an evidence that users are likely to be price sensitive. However, it can also be seen that there is still space to increase revenue. </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C: Better placement for graphs / alignment needed - get rid of the empty box\n",
    "\n",
    "# Next up, we will show how often various playlists come up in the data\n",
    "# Because we have so many playlists, we only display those who make up more than 2 % of the total data\n",
    "# https://stackoverflow.com/questions/37598665/how-to-plot-a-value-counts-in-pandas-that-has-a-huge-number-of-different-counts\n",
    "\n",
    "f, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "prob = data.playlist_name.value_counts(normalize=True)\n",
    "threshold = 0.02\n",
    "mask = prob > threshold\n",
    "tail_prob = prob.loc[~mask].sum()\n",
    "prob = prob.loc[mask]\n",
    "prob['other'] = tail_prob\n",
    "prob.plot(kind='bar', color=[\"#1aa64b\",\"#363837\"], ax=axes[0, 0])\n",
    "axes[0,0].set_title(\"Playlists Distribution\")\n",
    "\n",
    "# But here we show all artists who make up more than 5 % of total streams - 2 % would lead to an overcrowded graph\n",
    "prob = data.artist_name.value_counts(normalize=True)\n",
    "threshold = 0.05\n",
    "mask = prob > threshold\n",
    "tail_prob = prob.loc[~mask].sum()\n",
    "prob = prob.loc[mask]\n",
    "prob['other'] = tail_prob\n",
    "prob.plot(kind='bar', color=[\"#1aa64b\",\"#363837\"], ax=axes[0, 1])\n",
    "axes[0,1].set_title(\"Artists Distribution\")\n",
    "\n",
    "# And here we show all tracks that make up more than 1 % of total streams\n",
    "#prob = data.track_name.value_counts(normalize=True)\n",
    "#threshold = 0.01\n",
    "#mask = prob > threshold\n",
    "#tail_prob = prob.loc[~mask].sum()\n",
    "#prob = prob.loc[mask]\n",
    "#prob['other'] = tail_prob\n",
    "#prob.plot(kind='bar', color=[\"#1aa64b\",\"#363837\"], ax=axes[1, 0])\n",
    "#axes[1,0].set_title(\"Tracks Distribution\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b>Playlist distribution</b>\n",
    "\n",
    "<p>The Playlists suggested by the Warner Analysts (Hot Hits UK, Massive Dance Hits, The Indie List, New Music Friday) are not very popular, with the exception of Hot Hits UK, which indeed is the very most popular playlist. The others do not show up at all with the threshold of 2%, however, we see that the majority of playlists are small ones.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b>Artist distribution</b>\n",
    "\n",
    "<p>For artists, we see that only 6 artists have more than 5 % of total streams each, with Charlie Puth being the \"Most popular artist\" by far, followed by Dua Lipa and Lukas Graham.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b>Track distribution</b>\n",
    "\n",
    "<p>For tracks, we see that only 13 tracks have more than 2 % of total streams each, with \"7 Years\" being the \"Most popular Track\" by far. This is a Lukas Graham track. </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bianca \n",
    "# Now we want to visualize the time series distribution of streams using the 'date' column  \n",
    "\n",
    "# Create a DataFrame of the total number of streams per date  \n",
    "df_date = data['date'].value_counts()\n",
    "df_date=pd.DataFrame(df_date.sort_index())\n",
    "df_date.reset_index(level=df_date.index.names, inplace=True) \n",
    "df_date=df_date.rename(columns={\"index\": \"Date\"})\n",
    "df_date=df_date.rename(columns={\"date\":\"Number Of Streams\"}) \n",
    "\n",
    "# Create the bar graph to better visualize time series\n",
    "sns.set(style=\"whitegrid\")\n",
    "f, ax = plt.subplots(figsize=(6, 15))\n",
    "sns.barplot(x=(df_date['Number Of Streams']), y=df_date['Date'],\n",
    "            data=df_date,            \n",
    "            label=\"Number of streams by month\",\n",
    "            orient = \"h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<b> Time series distribution of streams </b>\n",
    "\n",
    "<p> We notice that the number of streams follows an upward trend. This indicates that Spotify, as expected, has raised in demand from year to year, seeing a significant increase in the last few years. However, there are some declines at certain dates, which shows that there might be a  degree of seasonality in the time series.</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    <u>Feature Engineering</u> \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding The Dependent Variable\n",
    "\n",
    "To facilitate data exploration, the successful artists will be identified at this stage, in order to understand their users' behaviours better.\n",
    "\n",
    "There is more than one way of adding the dependent variable. The following will use two methods. Two columns will be created: \n",
    "- \"successful\" - taking value of 1 for the rows containing artist name AND one of the 4 top playlist, otherwise 0\n",
    "- \"success\" - taking value of 1 for the rows containing artist name who showed up on one of the 4 top playlist at least once in the entire dataset, otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select relevant playlists \n",
    "successid=filtered_mapper[\"id\"].unique().tolist()\n",
    "\n",
    "# Define variable which looks at \"if artist && top playlist ==> successful, otherwise unsuccessful\"\n",
    "success = data[\"playlist_id\"].isin(successid)\n",
    "data[\"successful\"] = success\n",
    "\n",
    "#============second variable==============\n",
    "# Functon that retrieves a list with all successful artists (who showed up on a top playlist at least once)\n",
    "def get_successful_artists(all_artists):\n",
    "    records_successful_playlists = all_artists[all_artists['playlist_id'].isin(successid)]\n",
    "    return records_successful_playlists['artist_name'].unique()\n",
    "\n",
    "successful_artists = get_successful_artists(data)\n",
    "\n",
    "# Define variable which looks at \"if successful artist ==> successful, otherwise unsuccessful\"\n",
    "# Create column with dependent variable \"success\", which takes value of 1if the artist is in the successful list\n",
    "data['success'] = np.where(data['artist_name'].isin(successful_artists), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can have successful artists in both buckets, but only first is successfull artists in successfull playlists\n",
    "print('successful artists:', data.loc[data['successful'] == 1]['artist_name'].nunique())\n",
    "print('unsuccessful artists:', data.loc[data['successful'] == 0]['artist_name'].nunique())\n",
    "data['successful'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If artists is successfull, regardless of playlist, its 1\n",
    "print('successful artists:', data.loc[data['success'] == 1]['artist_name'].nunique())\n",
    "print('unsuccessful artists:', data.loc[data['success'] == 0]['artist_name'].nunique())\n",
    "data['success'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('seaborn-white')\n",
    "\n",
    "top_artists_df = data.loc[data['success'] == 1]['artist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "objects = top_artists_df.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = top_artists_df.values\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.title('Top 10 most streamed successful artists')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Successful Artists DF:\n",
    "successful_artists_df = data.loc[data['artist_name'].isin(successful_artists)]\n",
    "print('Total successful artists:', successful_artists_df['artist_name'].nunique())\n",
    "\n",
    "top_unpopular_playlists = successful_artists_df.loc[successful_artists_df['successful']==False]['playlist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "objects = top_unpopular_playlists.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = top_unpopular_playlists.values\n",
    "\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.title('Top 10 playlists most popular artists show up, different than \"The 4\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stronger recommendation based on other playlists - key playlists may not be relevant any more, many other playlists have way more streams\n",
    "top_unpopular_playlists = successful_artists_df.loc[successful_artists_df['success']==1]['playlist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "objects = top_unpopular_playlists.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = top_unpopular_playlists.values\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.title('Top Playlists of Successful Artists')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Hot Hits UK\", \"Massive Dance Hits\", \"The Indie List\", \"New Music Friday\"\n",
    "print('From a total of {} successful artists'.format(len(successful_artists)))\n",
    "print('Total successful artists showing up on Hot Hits UK:', successful_artists_df.loc[successful_artists_df['playlist_name'] == 'Hot Hits UK']['artist_name'].nunique())\n",
    "print('Total successful artists showing up on Massive Dance Hits:', successful_artists_df.loc[successful_artists_df['playlist_name'] == 'Massive Dance Hits']['artist_name'].nunique())\n",
    "print('Total successful artists showing up on The Indie List:', successful_artists_df.loc[successful_artists_df['playlist_name'] == 'The Indie List']['artist_name'].nunique())\n",
    "print('Total successful artists showing up on New Music Friday:', successful_artists_df.loc[successful_artists_df['playlist_name'] == 'New Music Friday']['artist_name'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_artists_frequency_playlists = successful_artists_df.groupby('playlist_name')['artist_name'].nunique().sort_values(ascending=False).reset_index(name='successful_artists')[:15]\n",
    "#some_df[:20]\n",
    "\n",
    "#successful_artists_frequency_playlists_chart = successful_artists_frequency_playlists['playlist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "\n",
    "objects = successful_artists_frequency_playlists.playlist_name\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = successful_artists_frequency_playlists.successful_artists\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Number of Unique Successful Artists')\n",
    "plt.title('Playlists which contain the highest number of distinct successful artists')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Cluser Map \n",
    "def plot_cluster(t,figsize=None):\n",
    "    cg = sns.clustermap(t,figsize=(10, 8), cmap=\"mako\", vmin=1)\n",
    "    plt.setp(cg.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "    return cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the playlists and artists by number of streams\n",
    "playlists_filtered = data.stream_source_uri.value_counts().head(20).keys().tolist()\n",
    "artists_filtered = data.artist_name.value_counts().head(20).keys().tolist()\n",
    "df = data[:]\n",
    "df['playlist_name'] = df.stream_source_uri.astype(str).str[-22:].map(playlist_mapper.set_index('id')['name'])\n",
    "df = df.dropna(subset=['playlist_name','artist_name'])\n",
    "df = df[(df.stream_source_uri.isin(playlists_filtered))&df.artist_name.isin(artists_filtered)]\n",
    "df = df.groupby(['artist_name','playlist_name']).size().unstack().fillna(0)\n",
    "plot_cluster(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Distribution of streams per hour for the key playlists</b>\n",
    "\n",
    "We observe that overall users tend to stream the key playlists mostly from 10 a.m. to 3 p.m. From 5 a.m. to 10 a.m the number of streams start to raise while from 3 p.m to 8 p.m the number of streams falls. During the late hours of the night there are not much streams. This is quite expected if we think of a normal person's day-to-day rutine.\n",
    "\n",
    "We should also look at each key playlist's hourly trend. The Hot Hits UK playlist has the biggest number of streams somewhere between 3 p.m. and 8 p.m., while it sees a small decrease in the number of streams from 10 a.m to 3 p.m. Other than that, it follows the overall hourly trend discussed before. The Massive Dance Hits playlist seems to count the biggest number of streams at 3 p.m. and it generally follows the overall hourly trend discussed before. The New Music Friday playlist seems to also count the biggest number of streams at 3 p.m. We also observe an increase in the number of streams after 8 p.m. which does not happen for the other key playlists. Other than that, it tends to follow the overall hourly trend discussed before, but it has (most) signifficant ups and downs in number of streams. The Indie list playlists seems to count the biggest number of streams at 10 a.m. Other than that, it follows the overall hourly trend discussed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Bianca \n",
    "# Next, we want to analyse the distribution of streams per hour for the key playlists \n",
    "\n",
    "# 4 key Playlists \n",
    "key_playlists = playlist_mapper[playlist_mapper[\"name\"].isin([\"Hot Hits UK\", \"Massive Dance Hits\", \"The Indie List\", \"New Music Friday\"])]\n",
    "data_key_playlists = data[data.stream_source_uri.astype(str).str[-22:].isin(key_playlists.id)]\n",
    "\n",
    "# Normalize to identify hourly trends \n",
    "key_playlists_streams_hour = data_key_playlists.groupby(['playlist_name','hour']).size().unstack().fillna(0).transpose()\n",
    "key_playlists_streams_hour.plot(kind='line',figsize=(12,6))\n",
    "\n",
    "# Visulaize the distribution of streams per hour (Normalized)\n",
    "key_playlists_streams_hour_norm = pd.DataFrame(scale(key_playlists_streams_hour))\n",
    "key_playlists_streams_hour_norm.columns = key_playlists_streams_hour.columns\n",
    "key_playlists_streams_hour_norm.plot(title='Distribution Of Streams Per Hour (Normalized)',kind='line',figsize=(12,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By <b>removing the entires containing successful artists in the Hot Hits UK</b>, we can see that the number of such unique artists does not drop. A possible explaination could be that Hot Hits UK selects the most popular artists who are frequent in other playlists. Therefore, HHUK would not be the most appropriate benchmark to compare successful artists against. By looking at this playlist, we might lose focus from others, which might be more associated with the actual success level of an artist. \n",
    "\n",
    "Therefore, moving forward, we will disregard the strems taking place on Hot Hits UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_artists_df_no_HHUK = successful_artists_df.loc[successful_artists_df.playlist_name != 'Hot Hits UK']\n",
    "successful_artists_df_no_HHUK.artist_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_playlists = successful_artists_df_no_HHUK.stream_source_uri.value_counts().head(20).keys().tolist()\n",
    "filter_artists = successful_artists_df_no_HHUK.artist_name.value_counts().head(20).keys().tolist()\n",
    "t = successful_artists_df_no_HHUK[:]\n",
    "t['playlist_name'] = t.stream_source_uri.astype(str).str[-22:].map(playlist_mapper.set_index('id')['name'])\n",
    "t = t.dropna(subset=['playlist_name','artist_name'])\n",
    "t = t[(t.stream_source_uri.isin(filter_playlists))&t.artist_name.isin(filter_artists)]\n",
    "t = t.groupby(['artist_name','playlist_name']).size().unstack().fillna(0)\n",
    "\n",
    "plot_cluster(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Artists Features\n",
    "Because in the above analysis, we learned that dropping Hot Hits UK streams would actually allow better visibility over artists' presence on other playlists, we will have to remove these entires in the entire dataset.\n",
    "\n",
    "We can look at an artist's success at multiple levels, vertically and horizontally.hence either to the actual number of streams and users it reached to, or to the playlists it reached or their history with Spotify. \n",
    "\n",
    "The 3 features suggested by Alaistar:\n",
    " - Stream count\n",
    " - Total Number of users\n",
    " - Passion Score\n",
    "\n",
    "Besides the 3 features suggested by Alaistar, we could also look into: \n",
    "- Rank based on all time streams --> AVERAGE STREAMING PER ARTISTS\n",
    "- Average it over few months (12)? \n",
    "- First time a track got played? - until it reached success?\n",
    "- Podcasts? - types/gendres?\n",
    "- How often a particular song is playes and shared? ~ nice to have but no data\n",
    "\n",
    "https://community.spotify.com/t5/Content-Questions/What-does-the-Popular-tracks-on-an-artist-page-mean-How-is-it/td-p/4405230\n",
    "\n",
    "Artists popularity is ranked by Spotify in several ways: \n",
    "- how often songs are played, therefore number of streams per song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Because in the above analysis, we learned that dropping Hot Hits UK streams would actually allow better \n",
    "visibility over artists' presence on other playlists, we will have to remove these entires in the entire dataset.'''\n",
    "df = data.loc[data.playlist_name != 'Hot Hits UK']\n",
    "\n",
    "# 1. TOTAL NUMBER OF STREAMS PER ARTIST\n",
    "artists_df = df['artist_name'].value_counts().to_frame().reset_index()\n",
    "artists_df = artists_df.rename(columns={\"index\" : \"artist_name\",\n",
    "                                         \"artist_name\" : \"ar_stream_count\"})\n",
    "# 2. TOTAL NUMBER OF USERS PER ARTIST\n",
    "df_artist_users = df.groupby('artist_name')['customer_id'].nunique().sort_values(ascending=False).reset_index(name='ar_users')\n",
    "\n",
    "# Merging the two dataframes related to streams and users into one\n",
    "artists_df = pd.merge(artists_df, df_artist_users, on='artist_name', how='outer')\n",
    "\n",
    "# 3. PASSION SCORE\n",
    "artists_df[\"ar_passion_score\"]= artists_df['ar_stream_count']/artists_df['ar_users']\n",
    "print(\"Range of users' passion score:\", artists_df[\"ar_passion_score\"].min(), artists_df[\"ar_passion_score\"].max())\n",
    "\n",
    "#==========NEW================\n",
    "# 4. AVERAGE TRACK STREAMING PER ARTISTS --> on average, how frequent songs are streamed, by looking at the time stamp, rather than the user id\n",
    "avg_streamed_tracks_per_artist = df.groupby(['artist_name', 'track_name'])['log_time'].nunique().groupby(['artist_name']).mean().sort_values(ascending = False).to_frame().reset_index().rename(columns={\"log_time\" : \"ar_avg_tracks_streaming\"})\n",
    "\n",
    "# Merging the two dataframes related to streams and users into one\n",
    "artists_df = pd.merge(artists_df, avg_streamed_tracks_per_artist, on='artist_name', how='outer')\n",
    "artists_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playlists Features\n",
    "Features suggested by Alaistar:\n",
    " - Prior Playlist Stream Counts\n",
    " - Prior Playlist Unique Users (Reach)\n",
    " - Prior Playlist Passion Score\n",
    "\n",
    "Other features:\n",
    "- Average number of streams per playlist - looking at the unique time stemp per every track, then averaging them per playlist\n",
    "- History of a playlist - first track that playlist got ever streamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TOTAL STREAMS PER PLAYLIST\n",
    "def playlist_avg_stream_counts(df):\n",
    "    stream_count_per_playlist_df = df['playlist_name'].value_counts().to_frame().reset_index()\n",
    "    return stream_count_per_playlist_df.rename(columns={\"index\" : \"playlist_name\", \n",
    "                                                        \"playlist_name\" : \"pl_stream_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. UNIQUE USERS PER PLAYLISTS\n",
    "def playlist_avg_number_of_users(df):\n",
    "    temp_df = df.groupby('playlist_name')['customer_id'].nunique().sort_values(ascending=False).reset_index(name='pl_users')\n",
    "    return temp_df[['playlist_name', 'pl_users']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merging the two dataframes related to playlists into one\n",
    "playlists_df = pd.merge(playlists_df, users_per_playlist, on='playlist_name', how='outer')\n",
    "\n",
    "# 3. PLAYLISTS PASSIONSCORE\n",
    "playlists_df[\"pl_passion_score\"]= playlists_df['pl_stream_count']/playlists_df['pl_users']\n",
    "print(\"Range of playlists' passion score:\", playlists_df[\"pl_passion_score\"].min(), playlists_df[\"pl_passion_score\"].max())\n",
    "\n",
    "#==========NEW==========\n",
    "# 4. AVERAGE STREAMING PER PLAYLIST --> on average, how often songs are streamed, by looking at the time stamp, rather than the user id\n",
    "avg_streamed_tracks_per_playlist = df.groupby(['playlist_name', 'track_name'])['log_time'].nunique().groupby(['playlist_name']).mean().sort_values(ascending = False).to_frame().reset_index().rename(columns={\"log_time\" : \"avg_tracks_streaming\"})\n",
    "\n",
    "# Merging the two dataframes related to streams and users into one\n",
    "playlists_df = pd.merge(playlists_df, avg_streamed_tracks_per_playlist, on='playlist_name', how='outer')\n",
    "playlists_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> History of a playlist</b>\n",
    "\n",
    "When evaluating playlists, we should also take into account their history within spotify. Ideally the older they are, the more reliability and consistency they would bring. These values could help us in creating better criteria for playlists assessment, which would subsequently help us better understand the level of success certain artists have. \n",
    "Because of the limited dataset, we will assess the history of a playlist based on the first track streamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First stream in every playlist\n",
    "playlists_name = playlist_mapper.name.unique()\n",
    "playlists_df_min_year = df.loc[df.playlist_name.isin(playlists_name)].groupby(['playlist_name'])['year'].min()\n",
    "playlists_df_min_year = playlists_df_min_year.to_frame().reset_index().rename(columns={\"year\": \"min_year\"})\n",
    "\n",
    "# Code for max\n",
    "# playlists_df_max_year = df.loc[df.playlist_name.isin(playlists_name)].groupby(['playlist_name'])['year'].min()\n",
    "# playlists_df_max_year = playlists_df_max_year.to_frame().reset_index().rename(columns={\"year\": \"max_year\"})\n",
    "# playlists_years = pd.merge(playlists_df_min_year, playlists_df_max_year, on='playlist_name', how='outer')\n",
    "\n",
    "#Create categorical data\n",
    "playlists_years = pd.get_dummies(playlists_df_min_year, columns=['min_year'], drop_first=True)\n",
    "\n",
    "#MERGING THE DF ABOVE\n",
    "playlists_df = pd.merge(playlists_df, playlists_years, on='playlist_name', how='outer')\n",
    "playlists_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Features\n",
    "1. Streaming of successful artists across different playlists\n",
    "2. Top playlists a particular artist was streamed the most in, different than the 4 playlists provided above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playlists_years_matrix(artists_name):\n",
    "    specific_artist_df = df.loc[df['artist_name']== artists_name]\n",
    "    pop_playlists = specific_artist_df['playlist_name'].value_counts()[:20].sort_values(ascending=False)\n",
    "    temp_dff = specific_artist_df.loc[specific_artist_df['playlist_name'].isin(pop_playlists.index)].groupby(['date','playlist_name']).size().unstack().fillna(0)\n",
    "    temp_dff = temp_dff.loc[:, (temp_dff != 0).any(axis=0)]\n",
    "    temp_dff = temp_dff.reindex(temp_dff.mean().sort_values(ascending = False).index, axis=1)\n",
    "    fig, ax = plt.subplots(figsize=(10,5))         # Sample figsize in inches\n",
    "    plot_title = 'Playlists Outreach for ' + artists_name\n",
    "    sns.heatmap(temp_dff.T, cmap = 'coolwarm', linewidths=.5, ax=ax).set_title(plot_title)\n",
    "\n",
    "def show_top_playlists(artist_name):\n",
    "    df_only_with_artist = df.loc[df['artist_name']== artist_name]\n",
    "    artist_in_top_playlists = df_only_with_artist.loc[df_only_with_artist['successful']==False]['playlist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "    fig = plt.figure()\n",
    "    objects = artist_in_top_playlists.keys()\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = artist_in_top_playlists.values\n",
    "    plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "    plt.yticks(y_pos, objects)\n",
    "    plt.xlabel('Number of Occurrences')\n",
    "    plot_title = 'Key Playlists Different than \"The 4\" for ' + artist_name\n",
    "    plt.title(plot_title)\n",
    "    plt.show()\n",
    "\n",
    "#For Testing   \n",
    "# playlists_years_matrix('dua lipa')\n",
    "# show_top_playlists('dua lipa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To get a better undertansing, 10 random artists were picked to be looked more closely into.\n",
    "# #DRILL DOWN INTO 10 RANDOM SUCCESSFUL ARTISTS \n",
    "# import random\n",
    "# from random import randint\n",
    "# x = random.sample(range(0, 82), 10)\n",
    "# pop_artists = df.loc[data['success'] == 1]['artist_name'].unique()\n",
    "# random_successful_artists = pop_artists[x]\n",
    "\n",
    "# for artist in random_successful_artists:\n",
    "#     playlists_years_matrix(artist)\n",
    "    \n",
    "# for artist in random_successful_artists:\n",
    "#     show_top_playlists(artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the above 10 randomly selected successful artists, we can notice that some artists become highly popular on various playlists, e.g. \"Eurovision Related\", \"Every UK Number 1\" or geners related e.g. pop, R&B. Therefore, the hypothesys is that playlists different than the 4 selected, can also be correlated to the likelyhood of an artist coming up on one of the 4 playlists; therefore, being popular on these playlists could be associated with \"success\".\n",
    "\n",
    "We can also notive that out of the 4 playlsts associated with artists success (\"Hot Hits UK\", \"Massive Dance Hits\", \"The Indie List\", \"New Music Friday\") are not the playlists where they are also the most streamed. When looking at top 20 playlists for 10 random successful artists, only 2 of them show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGGREGATE THE RESULTS OF ALL SUCCESSFUL ARTISTS\n",
    "successful_artist_df = df.loc[df['artist_name'].isin(successful_artists)]\n",
    "pop_playlists = successful_artist_df['playlist_name'].value_counts()[:20].sort_values(ascending=False)\n",
    "temp_dff = successful_artist_df.loc[successful_artist_df['playlist_name'].isin(pop_playlists.index)].groupby(['date','playlist_name']).size().unstack().fillna(0)\n",
    "temp_dff = temp_dff.loc[:, (temp_dff != 0).any(axis=0)]\n",
    "temp_dff = temp_dff.reindex(temp_dff.mean().sort_values(ascending = False).index, axis=1)\n",
    "fig, ax = plt.subplots(figsize=(10,5))         \n",
    "plot_title = 'Top Playlists Different than the Top 4' \n",
    "sns.heatmap(temp_dff.T, cmap = 'coolwarm', linewidths=.5, ax=ax).set_title(plot_title)\n",
    "\n",
    "\n",
    "artist_in_top_playlists = df.loc[df['successful']==False]['playlist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "fig = plt.figure()\n",
    "objects = artist_in_top_playlists.keys()\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = artist_in_top_playlists.values\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plot_title = 'Key Playlists Different than \"The 4\" '\n",
    "plt.title(plot_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the matrix, we can see that few playlists actually have a consistent high number of streams in general. Most of them present some level of seasonality, for example, \"Summer Hits\" had the highest number of streams in the summer of 2016. Therefore, we should look at them also across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Looking at most popular playlists over years')\n",
    "def plot_top_playlists_by_year(year):\n",
    "    artist_in_top_playlists = df.loc[df['year']==year]\n",
    "    artist_in_top_playlists = artist_in_top_playlists.loc[artist_in_top_playlists['successful']==False]['playlist_name'].value_counts()[:10].sort_values(ascending=True)\n",
    "    fig = plt.figure()\n",
    "    objects = artist_in_top_playlists.keys()\n",
    "    y_pos = np.arange(len(objects))\n",
    "    performance = artist_in_top_playlists.values\n",
    "    plt.barh(y_pos, performance, align='center', alpha=0.5, color = \"#1aa64b\")\n",
    "    plt.yticks(y_pos, objects)\n",
    "    plt.xlabel('Number of Occurrences')\n",
    "    plot_title = 'Key Playlists Different than \"The 4\" in ' + str(year)\n",
    "    plt.title(plot_title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_top_playlists_by_year(2015)\n",
    "plot_top_playlists_by_year(2016)\n",
    "plot_top_playlists_by_year(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these findings, we can observe there are several other playlists which contain a high number of streams of the successful artists and are fairly consistent over time. Surprisingly, but the other 3 playlists <b>\"Massive Dance Hits\", \"The Indie List\", \"New Music Friday\" </b> are not among them. Disregarding Hot Hits UK, we can notice that successful artists are the most streamed on these playlists: <b>'Today's Top Hits', 'Topsify UK Top 40', 'Freshness: Hot House Music', 'The Pop List'</b>. Therefore, similar to the top 4 \"successful\" playlists, we can label artists by whether they appeared on any of these playlists.\n",
    "\n",
    "To avoid any potential bias on those playlsts, we can select the following 4 playlists and keep track of those in a similar fashion: <b>'New Music Monday UK', 'Happy Hits!', 'Summer Hits', 'Top Tracks in the United Kingdom'</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_other_playlists_1 = ['Today\\'s Top Hits', 'Topsify UK Top 40', 'Freshness: Hot House Music', 'The Pop List']\n",
    "print(\"2nd Order Successful Playlists\")\n",
    "print(top_other_playlists_1)\n",
    "print()\n",
    "\n",
    "top_other_playlists_2 = ['New Music Monday UK', 'Happy Hits!', 'Summer Hits', 'Top Tracks in the United Kingdom']\n",
    "print(\"3rd Order Successful Playlists\")\n",
    "print(top_other_playlists_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_successful_artists_1(all_artists):\n",
    "    uccessful_playlists_1 = all_artists[all_artists['playlist_name'].isin(top_other_playlists_1)]\n",
    "    return uccessful_playlists_1['artist_name'].unique()\n",
    "\n",
    "def get_successful_artists_2(all_artists):\n",
    "    successful_playlists_2 = all_artists[all_artists['playlist_name'].isin(top_other_playlists_2)]\n",
    "    return successful_playlists_2['artist_name'].unique()\n",
    "\n",
    "successful_artists_1 = get_successful_artists_1(df)\n",
    "successful_artists_2 = get_successful_artists_2(df)\n",
    "\n",
    "# Create column with dependent variable \"success\", which takes value of 1if the artist is in the successful list\n",
    "df['success_1'] = np.where(df['artist_name'].isin(successful_artists_1), 1, 0)\n",
    "df['success_2'] = np.where(df['artist_name'].isin(successful_artists_2), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_successful_artists_df = df.loc[df.success == 1]\n",
    "\n",
    "temp_df_1 = temp_successful_artists_df.loc[temp_successful_artists_df['artist_name'].isin(successful_artists_1)]\n",
    "print('Out of 82 successful artists, {} showed up on top_other_playlists_1.'.format(temp_df_1['artist_name'].nunique()))\n",
    "\n",
    "temp_df_2 = temp_successful_artists_df.loc[temp_successful_artists_df['artist_name'].isin(successful_artists_2)]\n",
    "print('Out of 82 successful artists, {} showed up on top_other_playlists_2:'.format(temp_df_2['artist_name'].nunique()))\n",
    "\n",
    "### PLOT \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 10), dpi=80)\n",
    "\n",
    "labels = ['Yes', 'No']\n",
    "colors = [\"#1aa64b\",\"#363837\"]\n",
    "\n",
    "var1 = temp_df_1['artist_name'].nunique()\n",
    "sizes1 = [var1, 82-var1]\n",
    "ax1.set_title('Successful artists who showed up on top_other_playlists_1')\n",
    "ax1.pie(sizes1,  explode=(0.04,0), labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, pctdistance=1.2,labeldistance=1.4)\n",
    "\n",
    "var2 = temp_df_2['artist_name'].nunique()\n",
    "sizes2 = [var2, 82-var2]\n",
    "ax2.set_title('Successful artists who showed up on top_other_playlists_2')\n",
    "ax2.pie(sizes2,  explode=(0.04,0), labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, pctdistance=1.2,labeldistance=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #trying to see the overlap\n",
    "# temp_df_3 = temp_df_1.merge(temp_df_2, on='artist_name', how='left')\n",
    "# temp_df_3\n",
    "# fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "# var3 = temp_df_3['artist_name'].nunique()\n",
    "# sizes3 = [var3, 82-var3]\n",
    "# plt.title('Successful artists who showed up on both playlists')\n",
    "# plt.pie(sizes3,  explode=(0.04,0), labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, pctdistance=1.2,labeldistance=1.4)\n",
    "\n",
    "# plt.axis('equal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that majority of successful artists area streamed on these 2nd and 3rd order playlsits identified above. Therefore, we could consider looking at all 3 \"success\" variables and <b>compute our final dependent variable </b>.\n",
    "\n",
    "We bring together artists and their top 20 playlists, along with the details of those 20 playlists (excluding HHUK) as identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_pl = df.groupby(['artist_name','playlist_name']).size()\n",
    "\n",
    "# consider the top 20 playlists each artist has featured on\n",
    "df_ar_pl = df_ar_pl.groupby(level=0, group_keys=False).nlargest(20)\n",
    "\n",
    "df_ar_p1_reset = df_ar_pl.to_frame().reset_index()\n",
    "df_ar_p1_reset.rename(columns={0:'count'}, inplace=True)\n",
    "#df_ar_p1_reset\n",
    "\n",
    "#======================================================================================\n",
    "df_ar_p1_merge = df_ar_p1_reset.merge(playlists_df, on='playlist_name', how='inner')\n",
    "\n",
    "df_ar_p1_merge = df_ar_p1_merge.sort_values(by=['artist_name', 'count'], ascending=[1,0]) # ascending doesn't do anything \n",
    "#df_ar_p1_merge\n",
    "\n",
    "# add a column of the sum of playlist for each artist\n",
    "artist_sum = df_ar_p1_merge.groupby(['artist_name']).agg({'count':sum})\n",
    "\n",
    "df_ar_p1_merge = df_ar_p1_merge.merge(artist_sum,on='artist_name', how='inner', suffixes=('','_sum'))\n",
    "#df_ar_p1_merge\n",
    "\n",
    "#======================================================================================\n",
    "df_ar_p1_merge['weight_score'] = df_ar_p1_merge['count'] * df_ar_p1_merge['pl_passion_score'] / df_ar_p1_merge['count_sum']\n",
    "\n",
    "pl_avg = df_ar_p1_merge.groupby('artist_name').agg({'weight_score':sum})\n",
    "\n",
    "# drop old weight score, to avoid duplicate.\n",
    "df_ar_p1_merge.drop('weight_score', axis=1, inplace=True)\n",
    "\n",
    "df_ar_p1_merge = df_ar_p1_merge.merge(pl_avg, on='artist_name', how='inner')\n",
    "#df_ar_p1_merge\n",
    "\n",
    "#======================================================================================\n",
    "df_wght = df_ar_p1_merge[['artist_name','weight_score']]\n",
    "\n",
    "df_wght = df_wght.drop_duplicates()\n",
    "\n",
    "df_wght\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========Adding this variable to the artists dataframe =========\n",
    "artists_df = artists_df.merge(df_wght, on='artist_name', how='left')\n",
    "artists_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Playlist features</b>\n",
    "\n",
    "We decide to create a dataframe that contains streamcount, number of users and the passion score of each playlist and then calculate the weighted average of top 20 playlists' passion score for each artist. Thus, we calculate the streamcount of each playlist and create the dataframe 'playlists_df'. Then we find out the number of users of each playlist and add it to the dataframe 'playlists_df'. Similarly, we calculate the passion score of each playlist by dividing streamcount by number of users and add it to the dataframe 'playlists_df'. For calculating the weighted average of top 20 playlists' passion scores for each artist, we start by finding the top 20 stream count of playlists for each artists. Then we merge it with the relevant playlist passion score and calculate the sum of the different playlist's stream count for each artist. Here we calculate the weighted average for each playlist of each artist. For example, an artist might occur in many playlists, so the stream count and the passion score of this artist in each playlist is different. We sum the total stream count for each artist from all the playlists they occur and then use the weighted average method to see the score of one artist in a particular playlist. \n",
    "\n",
    "The formula is: Stream count * playlist passion score / the sum of the stream count of one artist in different playlists.\n",
    "\n",
    "Here we add the weighted average score to the dataframe. After that, we create a dataframe including the total weighted score of each artist and merge this into the dataframe 'df_artist'. This dataframe contains all the useful features variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-base features\n",
    "- Gender Percentage Breakdown\n",
    "- Age vector quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proof that there are missing values\n",
    "df.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender breakdown\n",
    "df_gender = df.copy()\n",
    "\n",
    "def gender_percentage(df_gen, gender='male'):\n",
    "    if len(df_gen)!=0:\n",
    "        # count the number of na\n",
    "        num_na = df_gen.gender.isna().sum()\n",
    "        \n",
    "        # assume half of nan gender as 'male' \n",
    "        df_gen = df_gen.drop_duplicates(subset=['customer_id'])\n",
    "        \n",
    "        perc = (df_gen[df_gen.gender==gender].shape[0] + num_na/2)/len(df_gen)\n",
    "        \n",
    "        return round(perc, 4)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "gender_per = df_gender.groupby('artist_name').apply(gender_percentage, gender='male')\n",
    "gender_per = gender_per.to_frame().reset_index()\n",
    "gender_per = gender_per.rename(columns={0: \"male_percentage\"})\n",
    "gender_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_artists_df = artists_df.merge(gender_per, on='artist_name', how='left')\n",
    "temp_artists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age breakdown\n",
    "\n",
    "# groups of age\n",
    "group_ages = [0,18,25,50,120]\n",
    "group_names = ['Teen','Youth','Adult','Senior']\n",
    "\n",
    "def age_group_per(df_new):\n",
    "    df_new = df_new.dropna(subset=['birth_year'])\n",
    "    \n",
    "    df_new['age'] = df_new.year - df_new.birth_year\n",
    "    \n",
    "    df_new = df_new`.drop_duplicates(subset=['customer_id'])\n",
    "    \n",
    "    categories = pd.cut(df_new['age'], group_ages, labels=group_names)\n",
    "    \n",
    "    cate_per = {y:x/categories.value_counts().sum() for x,y in zip(categories.value_counts(), categories.value_counts().keys())}\n",
    "\n",
    "    return pd.DataFrame([cate_per])\n",
    "\n",
    "age_per = df.groupby('artist_name').apply(age_group_per)\n",
    "\n",
    "# print(age_per)\n",
    "\n",
    "# avoid collinearity, abandon one group\n",
    "artists_df[['Youth','Adult','Senior']] = age_per.reset_index()[['Youth','Adult','Senior']]\n",
    "\n",
    "artists_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
